{
  "paragraphs": [
    {
      "text": "print(\"\"\"%html\n\u003ccenter\u003e\n    \u003ch1\u003eProgramación Distribuida sobre Grandes Volúmenes de Datos\u003c/h1\u003e\n\u003c/center\u003e\n\n\u003cbr\u003e\n\n\u003ch3 style\u003d\"text-align:center;\"\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    Facultad de Matemática Astronomía Física y Computación\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ca href\u003d\"http://www.unc.edu.ar\"\u003e\n    Universidad Nacional de Córdoba\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ccenter\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    \u003cimg src\u003d\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/comun/logo%20UNC%20FAMAF%202016.svg\" alt\u003d\"Drawing\" style\u003d\"width:50%;\"/\u003e\n    \u003c/a\u003e\n    \u003c/center\u003e\n\u003c/h3\u003e\n\n\u003ch4 style\u003d\"text-align:center;\"\u003e Damián Barsotti  \u003c/h4\u003e\n\n\u003cp style\u003d\"font-size:15px;\"\u003e\n    \u003cbr /\u003e\n        This work is licensed under a\n        \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003eCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\u003c/a\u003e.\n    \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003e\n        \u003cimg alt\u003d\"Creative Commons License\" style\u003d\"border-width:0;vertical-align:middle;float:right\" src\u003d\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /\u003e\n    \u003c/a\u003e\n\u003c/p\u003e\n\"\"\")",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ccenter\u003e\n    \u003ch1\u003eProgramación Distribuida sobre Grandes Volúmenes de Datos\u003c/h1\u003e\n\u003c/center\u003e\n\n\u003cbr\u003e\n\n\u003ch3 style\u003d\"text-align:center;\"\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    Facultad de Matemática Astronomía Física y Computación\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ca href\u003d\"http://www.unc.edu.ar\"\u003e\n    Universidad Nacional de Córdoba\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ccenter\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    \u003cimg src\u003d\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/comun/logo%20UNC%20FAMAF%202016.svg\" alt\u003d\"Drawing\" style\u003d\"width:50%;\"/\u003e\n    \u003c/a\u003e\n    \u003c/center\u003e\n\u003c/h3\u003e\n\n\u003ch4 style\u003d\"text-align:center;\"\u003e Damián Barsotti  \u003c/h4\u003e\n\n\u003cp style\u003d\"font-size:15px;\"\u003e\n    \u003cbr /\u003e\n        This work is licensed under a\n        \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003eCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\u003c/a\u003e.\n    \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003e\n        \u003cimg alt\u003d\"Creative Commons License\" style\u003d\"border-width:0;vertical-align:middle;float:right\" src\u003d\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /\u003e\n    \u003c/a\u003e\n\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890413_487809356",
      "id": "20161011-125025_834797080",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# Datasets/Dataframes\n\n* Spark permite interactuar con datos estructurados (Bases de Datos tabulares) o semiestructurados (JSON) con su componente **Spark SQL**.\n* Sus interfaces son SQL y Dataset/Dataframe API.\n* En desarrollo: usar Spark versión 2.0.0+.\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch1\u003eDatasets/Dataframes\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eSpark permite interactuar con datos estructurados (Bases de Datos tabulares) o semiestructurados (JSON) con su componente \u003cstrong\u003eSpark SQL\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eSus interfaces son SQL y Dataset/Dataframe API.\u003c/li\u003e\n\u003cli\u003eEn desarrollo: usar Spark versión 2.0.0+.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890422_498197576",
      "id": "20161011-125142_1705237118",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "print(s\"\"\"%html\n\u0026nbsp;\n\u003cimg src\u003d\"$baseDir/unified_stack.png\" alt\u003d\"Drawing\" style\u003d\"width: 100%;\"/\u003e\n\"\"\")\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u0026nbsp;\n\u003cimg src\u003d\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/08_datasets_dataframes/unified_stack.png\" alt\u003d\"Drawing\" style\u003d\"width: 100%;\"/\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890424_495889083",
      "id": "20161011-132101_236091967",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sc.version\n\nrequire(sc.version.replace(\".\", \"\").substring(0,3).toInt \u003e\u003d 200, \"Spark 2.0.0+ es requerido para este notebook. Puede usar Zeppelin 0.6.1+\")\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 4:58:48 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890426_496658581",
      "id": "20161011-133444_1783461732",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 4:58:48 PM",
      "dateFinished": "Oct 6, 2017 4:58:58 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "API 2.0.x",
      "text": "print(s\"\"\"%html\n\u0026nbsp;\n\u003cimg src\u003d\"$baseDir/dataset_dataframe_unificado.png\" alt\u003d\"Drawing\" style\u003d\"width: 100%;\"/\u003e\n\"\"\")\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u0026nbsp;\n\u003cimg src\u003d\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/08_datasets_dataframes/dataset_dataframe_unificado.png\" alt\u003d\"Drawing\" style\u003d\"width: 100%;\"/\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890428_494350087",
      "id": "20161011-134614_1124280099",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### SparkSession\n\nAntes se usaba `SparkConf`, `SparkContext` y `SQLContext` para interactuar con Spark y con su componente SQL:\n\n```scala\nval sparkConf \u003d new SparkConf().setAppName(\"SparkSessionZipsExample\").setMaster(\"local\")\n// Se usaba SparkContext para acceder otros contextos como SQLContext\nval sc \u003d new SparkContext(sparkConf).set(\"spark.some.config.option\", \"some-value\")\nval sqlContext \u003d new org.apache.spark.sql.SQLContext(sc)\n```\n\nEn Spark 2.0 se utiliza un único punto de comunicación: `SparkSession`:\n\n(más información en [How to use SparkSession in Apache Spark 2.0](https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html) y [Apache Spark 2.0 Examples](https://docs.cloud.databricks.com/docs/latest/sample_applications/04%20Apache%20Spark%202.0%20Examples/01%20SparkSession.html))\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eSparkSession\u003c/h3\u003e\n\u003cp\u003eAntes se usaba \u003ccode\u003eSparkConf\u003c/code\u003e, \u003ccode\u003eSparkContext\u003c/code\u003e y \u003ccode\u003eSQLContext\u003c/code\u003e para interactuar con Spark y con su componente SQL:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003eval sparkConf \u003d new SparkConf().setAppName(\"SparkSessionZipsExample\").setMaster(\"local\")\n// Se usaba SparkContext para acceder otros contextos como SQLContext\nval sc \u003d new SparkContext(sparkConf).set(\"spark.some.config.option\", \"some-value\")\nval sqlContext \u003d new org.apache.spark.sql.SQLContext(sc)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEn Spark 2.0 se utiliza un único punto de comunicación: \u003ccode\u003eSparkSession\u003c/code\u003e:\u003c/p\u003e\n\u003cp\u003e(más información en \u003ca href\u003d\"https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\"\u003eHow to use SparkSession in Apache Spark 2.0\u003c/a\u003e y \u003ca href\u003d\"https://docs.cloud.databricks.com/docs/latest/sample_applications/04%20Apache%20Spark%202.0%20Examples/01%20SparkSession.html\"\u003eApache Spark 2.0 Examples\u003c/a\u003e)\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890431_494734836",
      "id": "20161011-174856_51715674",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "SparkSession",
      "text": "import org.apache.spark.sql.SparkSession\n\nval spark \u003d SparkSession\n  .builder()\n  .appName(\"Spark ejemplo\")\n  .config(\"spark.some.config.option\", \"algun-valor\")\n  .getOrCreate()\n\nval sc \u003d spark.sparkContext\n\n// Para conversion implícita, por ej entre RDD a DataFrames\nimport spark.implicits._\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:09:22 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890433_184627223",
      "id": "20161011-133634_238324561",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:09:22 PM",
      "dateFinished": "Oct 6, 2017 5:09:32 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## ~.- Datasets\n\n* Contenedor (collection)  Inmutable de objetos (\u003d RDD).\n* Mapeados a un esquema relacional: datos organizados en columnas (!\u003d RDD) con nombres (tablas).\n* Fuertemente tipados.\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch2\u003e~.- Datasets\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eContenedor (collection)  Inmutable de objetos (\u003d RDD).\u003c/li\u003e\n\u003cli\u003eMapeados a un esquema relacional: datos organizados en columnas (!\u003d RDD) con nombres (tablas).\u003c/li\u003e\n\u003cli\u003eFuertemente tipados.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890434_185781469",
      "id": "20161011-154647_1740150496",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Eficiencia\n\nLos RDD tienen el overhead de la *serialización* (aunque use *Kryo*):\n* cuando los objetos se transfieren (red) y guardan (disco)\n* overhead de garbage collector de la JVM\n\nLos Datasets solucionan estos problemas:\n* Serializa a binario usando **encoders**\n    - parte del proyecto Tungsten\n    - permite operaciones sin deserializar\n    - corre *off-heap* (sin garbage collection)\n    - código para serialización generado en forma dinámica\n* Con la información de la estructura (*schema*) Spark hace optimizaciones.\n    - Usa *Catalyst optimizer*.\n    - Transfiere solo columnas usadas, no objetos enteros (relational query plan).\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eEficiencia\u003c/h3\u003e\n\u003cp\u003eLos RDD tienen el overhead de la \u003cem\u003eserialización\u003c/em\u003e (aunque use \u003cem\u003eKryo\u003c/em\u003e):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecuando los objetos se transfieren (red) y guardan (disco)\u003c/li\u003e\n\u003cli\u003eoverhead de garbage collector de la JVM\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLos Datasets solucionan estos problemas:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSerializa a binario usando \u003cstrong\u003eencoders\u003c/strong\u003e\u003cul\u003e\n\u003cli\u003eparte del proyecto Tungsten\u003c/li\u003e\n\u003cli\u003epermite operaciones sin deserializar\u003c/li\u003e\n\u003cli\u003ecorre \u003cem\u003eoff-heap\u003c/em\u003e (sin garbage collection)\u003c/li\u003e\n\u003cli\u003ecódigo para serialización generado en forma dinámica\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCon la información de la estructura (\u003cem\u003eschema\u003c/em\u003e) Spark hace optimizaciones.\u003cul\u003e\n\u003cli\u003eUsa \u003cem\u003eCatalyst optimizer\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eTransfiere solo columnas usadas, no objetos enteros (relational query plan).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890437_183088227",
      "id": "20161011-174737_215333010",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "WordCount con RDD",
      "text": "val lines \u003d sc.textFile(\"README.md\")\nval words \u003d lines\n  .flatMap(_.split(\" \"))\n  .filter(_ !\u003d \"\")\n\nval counts \u003d words\n    .groupBy(_.toLowerCase)\n    .map(w \u003d\u003e (w._1, w._2.size))\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:01:01 PM",
      "config": {
        "lineNumbers": true,
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890438_184242474",
      "id": "20161011-173655_294413553",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:01:01 PM",
      "dateFinished": "Oct 6, 2017 5:01:03 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "WordCount con Dataset",
      "text": "val lines \u003d spark.read.text(\"README.md\").as[String]\n\nval words \u003d lines\n  .flatMap(_.split(\" \"))\n  .filter(_ !\u003d \"\")\n\nwords.show(2)\n\nprintln(\"El schema de words es:\")\nwords.printSchema()\n\nval counts \u003d words.groupBy(\"value\").count\n\ncounts.show(2)",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:01:11 PM",
      "config": {
        "lineNumbers": true,
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890440_181933980",
      "id": "20161011-190925_989390669",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:01:11 PM",
      "dateFinished": "Oct 6, 2017 5:01:15 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "print(s\"\"\"%html\n\u0026nbsp;\n\u003cimg src\u003d\"$baseDir/Distributed-Wordcount-Chart.png\" alt\u003d\"Drawing\" style\u003d\"width: 100%;\"/\u003e\n\"\"\")",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u0026nbsp;\n\u003cimg src\u003d\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/08_datasets_dataframes/Distributed-Wordcount-Chart.png\" alt\u003d\"Drawing\" style\u003d\"width: 100%;\"/\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890443_182318729",
      "id": "20161011-201107_202795512",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "print(s\"\"\"%html\n\u0026nbsp;\n\u003cimg src\u003d\"$baseDir/Memory-Usage-when-Caching-Chart.png\" alt\u003d\"Drawing\" style\u003d\"width: 100%;\"/\u003e\n\"\"\")",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u0026nbsp;\n\u003cimg src\u003d\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/08_datasets_dataframes/Memory-Usage-when-Caching-Chart.png\" alt\u003d\"Drawing\" style\u003d\"width: 100%;\"/\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890445_180010236",
      "id": "20161011-201419_1048673058",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Creación de Datasets\n\nSe pueden crear desde una secuencia con `toDS()`:",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eCreación de Datasets\u003c/h3\u003e\n\u003cp\u003eSe pueden crear desde una secuencia con \u003ccode\u003etoDS()\u003c/code\u003e:\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890447_180779734",
      "id": "20161011-193616_142188090",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Se pueden crear desde una secuencia con toDS():",
      "text": "val dataset \u003d Seq(1, 2, 3).toDS()\ndataset.show()\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:03:13 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890448_191167954",
      "id": "20161011-193803_2028261146",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:03:13 PM",
      "dateFinished": "Oct 6, 2017 5:03:14 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Tungsten en acción",
      "text": "val intsMM \u003d 1 to math.pow(10, 6).toInt\nsc.parallelize(intsMM).setName(\"enteros\").cache.count\n// Ver sparkui storage. Descomentar proxima linea y ver de nuevo\n//intsMM.toDS.cache.count\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:12:29 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890450_191937452",
      "id": "20161017-104100_1861019655",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:11:09 PM",
      "dateFinished": "Oct 6, 2017 5:11:19 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "{\nval uiHost \u003d sc.getConf.getOption(\"spark.driver.host\").getOrElse(\"localhost\")\nval uiPort \u003d sc.getConf.getOption(\"spark.ui.port\").getOrElse(\"4040\")\nprint(s\"\"\"\n%html\n\u003cbr\u003e\nVer resultado en Spark UI Storage\n\u003ca href\u003d\"http://$uiHost:$uiPort/storage\"\u003ehttp://$uiHost(host):$uiPort(port)/storage\u003c/a\u003e\n\"\"\")\n}",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:05:22 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\n"
          },
          {
            "type": "HTML",
            "data": "\u003cbr\u003e\nVer resultado en Spark UI Storage\n\u003ca href\u003d\"http://200.16.17.38:4040/storage\"\u003ehttp://200.16.17.38(host):4040(port)/storage\u003c/a\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890451_191552703",
      "id": "20161017-104452_771251326",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:05:22 PM",
      "dateFinished": "Oct 6, 2017 5:05:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPara poner nombres a las columnas se puede crear una secuencia de una `case class`.\nEsto indica el tipo del Dataset.",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003ePara poner nombres a las columnas se puede crear una secuencia de una \u003ccode\u003ecase class\u003c/code\u003e.\n\u003cbr  /\u003eEsto indica el tipo del Dataset.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890453_189244209",
      "id": "20161011-193932_1234675334",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "case class Persona(nombre: String, edad: Int)\n\nimport org.apache.spark.sql.Dataset\n\nval personaDS : Dataset[Persona] \u003d Seq(Persona(\"Pepe\", 33), Persona(\"Lucio\", 32), Persona(\"Kuka\", 62)).toDS()\npersonaDS.show()\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:12:43 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890456_188089963",
      "id": "20161011-194123_1130859649",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:12:43 PM",
      "dateFinished": "Oct 6, 2017 5:12:45 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nPara leer el Dataset de un archivo estructurado también hay que señalar el tipo:",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003ePara leer el Dataset de un archivo estructurado también hay que señalar el tipo:\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890457_187705214",
      "id": "20161012-152737_362825205",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "case class Persona(age: Long, name: String)\nval ds \u003d spark.read.json(\"doc/people.json\").as[Persona]\n\nds.show()",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:13:03 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890459_188474712",
      "id": "20161012-152826_1162252757",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:13:03 PM",
      "dateFinished": "Oct 6, 2017 5:13:04 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nTambién se puede convertir un RDD a un Dataset:",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eTambién se puede convertir un RDD a un Dataset:\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890461_186166218",
      "id": "20161011-194558_2103618135",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val rdd \u003d sc.parallelize(Seq((1, \"Hola\"), (2, \"que tal\")))\nval integerDS \u003d rdd.toDS()\nintegerDS.show()\n//notar el nombre de las columnas",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:13:20 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890462_187320465",
      "id": "20161011-194820_1975808290",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:13:20 PM",
      "dateFinished": "Oct 6, 2017 5:13:21 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Operaciones de Base de Datos en Datasets\n\nEl próximo ejemplo muestra:\n* Union entre Datasets\n* Inner join con una condición\n* Agrupamiento por una columna\n* Programación de una aggregation (promedio) en el DS agrupado.\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eOperaciones de Base de Datos en Datasets\u003c/h3\u003e\n\u003cp\u003eEl próximo ejemplo muestra:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUnion entre Datasets\u003c/li\u003e\n\u003cli\u003eInner join con una condición\u003c/li\u003e\n\u003cli\u003eAgrupamiento por una columna\u003c/li\u003e\n\u003cli\u003eProgramación de una aggregation (promedio) en el DS agrupado.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890464_172700007",
      "id": "20161011-195456_566912833",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\nimport org.apache.spark.sql.Dataset\n\ncase class Empleado(nombre: String, edad: Int, departamentoId: Int, salario: Double)\ncase class Departamento(id: Int, nombre: String)\n\ncase class Registro(nombre: String, edad: Int, salario: Double, departamentoId: Int, departamentoNombre: String)\ncase class Resultado(departamentoId: Int, departamentoNombre: String, promSalario: Double)\n\nval empleadoDS1 : Dataset[Empleado] \u003d \n    sc.parallelize(Seq(Empleado(\"Pepe\", 22, 1, 100000.0), Empleado(\"Cacho\", 33, 2, 93000.0), Empleado(\"Kuka\", 35, 2, 89999.0), Empleado(\"Porota\", 39, 3, 120000.0))).toDS()\n\nval empleadoDS2 : Dataset[Empleado] \u003d \n    sc.parallelize(Seq(Empleado(\"Juancito\", 26, 1, 990000.0), Empleado(\"Pancho\", 38, 3, 115000.0))).toDS()\n\nval departamentoDS : Dataset[Departamento] \u003d \n    sc.parallelize(Seq(Departamento(1, \"Engeniería\"), Departamento(2, \"Fiesta\"), Departamento(3, \"Ventas\"))).toDS()\n\nval empleadoDS : Dataset[Empleado] \u003d empleadoDS1.union(empleadoDS2)\n\nval empleadoDeptoDS : Dataset[(Empleado, Departamento)] \u003d \n    empleadoDS.joinWith(departamentoDS, $\"departamentoId\" \u003d\u003d\u003d $\"id\" \u0026\u0026 $\"id\" \u003d!\u003d 1 , \"inner\")\n\nval registroDS : Dataset[Registro] \u003d \n    empleadoDeptoDS.map(reg \u003d\u003e Registro(reg._1.nombre, reg._1.edad, reg._1.salario, reg._1.departamentoId, reg._2.nombre))\n\n// ver tipo de registroGS\nval registroGS \u003d registroDS.filter(_.edad \u003e 25)\n                           .groupBy($\"departamentoId\", $\"departamentoNombre\")\n\n// Hay que ponerle nombre a la nueva columna y tipo al resultado:\nval promedioSalarioDS : Dataset[Resultado] \u003d registroGS.agg(avg($\"salario\").as(\"promSalario\")).as[Resultado]\n\n// Más facil con Dataframes\n//val promedioSalarioDF \u003d registroGS.avg(\"salario\")\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:17:44 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "lineNumbers": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890465_172315258",
      "id": "20161011-195903_867532839",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:17:44 PM",
      "dateFinished": "Oct 6, 2017 5:17:47 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "promedioSalarioDS.show()",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:18:01 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890467_173084756",
      "id": "20161011-202929_1635567143",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:18:01 PM",
      "dateFinished": "Oct 6, 2017 5:18:04 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nMas información en \n* [API Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)\n* [Column Class](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column)\n* [Function Reference](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)\n* [Doc Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html)",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eMas información en\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\"\u003eAPI Dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\"\u003eColumn Class\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"\u003eFunction Reference\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href\u003d\"http://spark.apache.org/docs/latest/sql-programming-guide.html\"\u003eDoc Spark SQL\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890468_171161011",
      "id": "20161012-103356_1938807399",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## ~.- Dataframes\n\n* Contenedor (collection) inmutable de objetos (\u003d RDD).\n* Datos organizados en columnas (!\u003d RDD) con nombres (tablas).\n* Ahora llamados *datasets no tipados*:\n  ```scala\n    type DataFrame \u003d Dataset[Row]\n  ```\n* Idea de [Python Panda dataframes](http://pandas.pydata.org/pandas-docs/stable/dsintro.html).\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch2\u003e~.- Dataframes\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eContenedor (collection) inmutable de objetos (\u003d RDD).\u003c/li\u003e\n\u003cli\u003eDatos organizados en columnas (!\u003d RDD) con nombres (tablas).\u003c/li\u003e\n\u003cli\u003eAhora llamados \u003cem\u003edatasets no tipados\u003c/em\u003e:\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003etype DataFrame \u003d Dataset[Row]\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003eIdea de \u003ca href\u003d\"http://pandas.pydata.org/pandas-docs/stable/dsintro.html\"\u003ePython Panda dataframes\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890469_170776262",
      "id": "20161011-151353_1348215106",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Creación\n\n* Se puede crear desde **RDD** o **Dataset** con `.toDF()`.\n* Puede leer desde archivos con formato estructurado gracias a **conectores**:\n    - JSON\n    - parquet\n    - JDBC\n    - ORC\n    - Hive\n    - [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n    - CSV\n    - Otros",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:20:23 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eCreación\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003eSe puede crear desde \u003cstrong\u003eRDD\u003c/strong\u003e o \u003cstrong\u003eDataset\u003c/strong\u003e con \u003ccode\u003e.toDF()\u003c/code\u003e.\u003c/li\u003e\n  \u003cli\u003ePuede leer desde archivos con formato estructurado gracias a \u003cstrong\u003econectores\u003c/strong\u003e:\n    \u003cul\u003e\n      \u003cli\u003eJSON\u003c/li\u003e\n      \u003cli\u003eparquet\u003c/li\u003e\n      \u003cli\u003eJDBC\u003c/li\u003e\n      \u003cli\u003eORC\u003c/li\u003e\n      \u003cli\u003eHive\u003c/li\u003e\n      \u003cli\u003e\u003ca href\u003d\"https://www.csie.ntu.edu.tw/~cjlin/libsvm/\"\u003elibsvm\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003eCSV\u003c/li\u003e\n      \u003cli\u003eOtros\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890471_171545760",
      "id": "20161017-105827_1416055376",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:20:18 PM",
      "dateFinished": "Oct 6, 2017 5:20:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Creación",
      "text": "val df \u003d spark.read.json(\"doc/people.json\")\n\n// Displays the content of the DataFrame to stdout\ndf.show()",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:20:30 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890472_169622015",
      "id": "20161011-135254_1312936627",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:20:30 PM",
      "dateFinished": "Oct 6, 2017 5:20:30 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Operaciones\n\nTiene operaciones de Base de Datos como project (select), filter, intersect, join, group, sort, aggregate.",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eOperaciones\u003c/h3\u003e\n\u003cp\u003eTiene operaciones de Base de Datos como project (select), filter, intersect, join, group, sort, aggregate.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890474_170391513",
      "id": "20161017-111605_1975080126",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Selecciona todo incrementando la edad\ndf.select($\"name\", $\"age\" + 1).show()\n\n// Selecciona personas con mas de 21 años\ndf.filter($\"age\" \u003e 21).show()\n\n// Cuenta personas por edad\ndf.groupBy(\"age\").count().show()\n\ndf.groupBy(\"age\").count().explain(true)\n\n// Convierte a Dataset denotando el tipo\nval ds \u003d df.as[(Long,String)]\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:20:55 PM",
      "config": {
        "colWidth": 8.0,
        "editorMode": "ace/mode/scala",
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true,
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890475_170006764",
      "id": "20161011-151030_1991021842",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:20:55 PM",
      "dateFinished": "Oct 6, 2017 5:20:56 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nMás operaciones en [DataFrame Function Reference](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 4.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eMás operaciones en \u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\"\u003eDataFrame Function Reference\u003c/a\u003e\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890476_168083020",
      "id": "20161011-151319_365789649",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Lectura/escritura\n\nPor defecto se lee/escribe en formato parquet:\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eLectura/escritura\u003c/h3\u003e\n\u003cp\u003ePor defecto se lee/escribe en formato parquet:\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890478_168852518",
      "id": "20161017-163502_615485385",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val usersDF \u003d spark.read.load(\"doc/users.parquet\")\nusersDF.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:21:18 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890479_168467769",
      "id": "20161017-163706_890748000",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:21:18 PM",
      "dateFinished": "Oct 6, 2017 5:21:19 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Opciones manuales",
      "text": "val peopleDF \u003d spark.read.format(\"json\").load(\"doc/people.json\")\n\nimport org.apache.spark.sql.SaveMode\n\npeopleDF.select(\"name\", \"age\").write.format(\"parquet\").mode(SaveMode.Overwrite).save(\"namesAndAges.parquet\")\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:21:40 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890480_178855989",
      "id": "20161017-164205_12539889",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:21:41 PM",
      "dateFinished": "Oct 6, 2017 5:21:41 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nMas información en [API DataFrameWriter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter)",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cp\u003eMas información en \u003ca href\u003d\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter\"\u003eAPI DataFrameWriter\u003c/a\u003e\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890481_178471240",
      "id": "20161017-165144_702288951",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Schema\n\nPara Dataframes (Datasets no tipados) Spark SQL infiere el schema o se puede indicar de forma explícita.",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch3\u003eSchema\u003c/h3\u003e\n\u003cp\u003ePara Dataframes (Datasets no tipados) Spark SQL infiere el schema o se puede indicar de forma explícita.\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890483_179240738",
      "id": "20161017-112943_962256942",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Schema automático",
      "text": "import spark.implicits._\n\nval df \u003d spark.read.json(\"doc/people.json\")\n\n// Muestra el schema de la tabla\n// También funciona con Datasets\ndf.printSchema()\n\n// Selecciona solo la columna \"name\"\ndf.select(\"name\").show()\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:22:38 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890484_177316993",
      "id": "20161011-145810_1731487287",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:22:38 PM",
      "dateFinished": "Oct 6, 2017 5:22:38 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Schema explícito",
      "text": "import org.apache.spark.sql.types._\n\n// Crea un RDD\nval peopleRDD \u003d spark.sparkContext.textFile(\"doc/people.txt\")\n\n// Se encodea el schema en un string auxiliar\nval schemaString \u003d \"name age\"\n\n// Se genera el schema según el string\nval fields \u003d schemaString.split(\" \")\n  .map(fieldName \u003d\u003e StructField(fieldName, StringType, nullable \u003d true))\nval schema \u003d StructType(fields)\n\n// Convierte el RDD de String a Row\nimport org.apache.spark.sql.Row\nval rowRDD \u003d peopleRDD\n  .map(_.split(\",\"))\n  .map(attributes \u003d\u003e Row(attributes(0), attributes(1).trim))\n\n// Aplica el schema al RDD creando el Dataframe\nval peopleDF \u003d spark.createDataFrame(rowRDD, schema)\n\n// Creates a temporary view using the DataFrame\npeopleDF.printSchema()\npeopleDF.dtypes",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:22:41 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890485_176932245",
      "id": "20161017-113138_160353802",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:22:41 PM",
      "dateFinished": "Oct 6, 2017 5:22:42 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## ~.- SQL\n\n`SparkSession` permite ejecutar queries SQL:",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch2\u003e~.- SQL\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003eSparkSession\u003c/code\u003e permite ejecutar queries SQL:\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890486_178086491",
      "id": "20161011-154344_2118638610",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val df \u003d spark.read.json(\"doc/people.json\")\n\n// Primero hay que registrar el DataFrame como un view de SQL\ndf.createOrReplaceTempView(\"people\")\n\nval sqlDF \u003d spark.sql(\"SELECT * FROM people WHERE age \u003e 20\")\nsqlDF.show()\n",
      "user": "anonymous",
      "dateUpdated": "Oct 6, 2017 5:22:58 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "df: org.apache.spark.sql.DataFrame \u003d [age: bigint, name: string]\nsqlDF: org.apache.spark.sql.DataFrame \u003d [age: bigint, name: string]\n+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890487_177701742",
      "id": "20161011-154515_480460035",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "dateStarted": "Oct 6, 2017 5:22:58 PM",
      "dateFinished": "Oct 6, 2017 5:22:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## ~.- Ventajas/desventajas de las diferentes APIs",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ch2\u003e~.- Ventajas/desventajas de las diferentes APIs\u003c/h2\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890488_175777998",
      "id": "20161017-123303_1620980443",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "APIs tipadas y no tipadas",
      "text": "print(s\"\"\"\n%table\nLenguaje\\t Abstracción Principal\nScala \\t Dataset[T] y Dataframe (Datset[Row])\nJava  \\t Dataset[T]\nPython \\t Dataframe\nR \\t Dataframe\n\"\"\")\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [
                {
                  "name": "Lenguaje",
                  "index": 0.0,
                  "aggr": "sum"
                }
              ],
              "values": [
                {
                  "name": " Abstracción Principal",
                  "index": 1.0,
                  "aggr": "sum"
                }
              ],
              "groups": [],
              "scatter": {
                "xAxis": {
                  "name": "Lenguaje",
                  "index": 0.0,
                  "aggr": "sum"
                },
                "yAxis": {
                  "name": " Abstracción Principal",
                  "index": 1.0,
                  "aggr": "sum"
                }
              }
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "Lenguaje\t Abstracción Principal\nScala \t Dataset[T] y Dataframe (Datset[Row])\nJava  \t Dataset[T]\nPython \t Dataframe\nR \t Dataframe\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890489_175393249",
      "id": "20161017-123522_72698600",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Detección de errores",
      "text": "print(s\"\"\"%html\n\u0026nbsp;\n\u003cimg src\u003d\"$baseDir/type-safety-spectrum.png\" alt\u003d\"Drawing\" style\u003d\"width: 100%;\"/\u003e\n\"\"\")",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 6.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u0026nbsp;\n\u003cimg src\u003d\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/08_datasets_dataframes/type-safety-spectrum.png\" alt\u003d\"Drawing\" style\u003d\"width: 100%;\"/\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890490_176547496",
      "id": "20161017-124430_789707578",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Cuando usar Datasets Dataframes o RDD",
      "text": "%md\n* Si se necesita expresiones de alto nivel, filters, maps, aggregations, promedios, sumatorias, queries SQL, acceso por columna y funciones lambda sobre datos semiestructurados\n    - para desarrollar aplicaciones finales (Data Ingeeniering) usar **Datasets**.\n    - para análisis interactivo (Data Scientist) usar **Dataframes**. \n* Si se necesita mayor seguridad de tipos chequeandolos a tiempo de compilación, objetos JVM, beneficios de optimización Catalyst y código eficiente con Tungsten usar **Datasets**.\n* Si se quiere una API unificada a traves de la la librerías Spark usar **DataFrames** o **Datasets**.\n* Si se quiere trabajar en R no queda otra que usar **DataFrames**.\n* Si se quiere trabajar en Python no queda otra que usar **DataFrames** y recurrir a **RDDs** si se necesita mayor control.\n",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cul\u003e\n\u003cli\u003eSi se necesita expresiones de alto nivel, filters, maps, aggregations, promedios, sumatorias, queries SQL, acceso por columna y funciones lambda sobre datos semiestructurados\u003cul\u003e\n\u003cli\u003epara desarrollar aplicaciones finales (Data Ingeeniering) usar \u003cstrong\u003eDatasets\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003epara análisis interactivo (Data Scientist) usar \u003cstrong\u003eDataframes\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSi se necesita mayor seguridad de tipos chequeandolos a tiempo de compilación, objetos JVM, beneficios de optimización Catalyst y código eficiente con Tungsten usar \u003cstrong\u003eDatasets\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eSi se quiere una API unificada a traves de la la librerías Spark usar \u003cstrong\u003eDataFrames\u003c/strong\u003e o \u003cstrong\u003eDatasets\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eSi se quiere trabajar en R no queda otra que usar \u003cstrong\u003eDataFrames\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eSi se quiere trabajar en Python no queda otra que usar \u003cstrong\u003eDataFrames\u003c/strong\u003e y recurrir a \u003cstrong\u003eRDDs\u003c/strong\u003e si se necesita mayor control.\u003c/li\u003e\n\u003c/ul\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890491_176162747",
      "id": "20161017-124933_322133526",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Fin",
      "text": "val baseDir\u003d\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/08_datasets_dataframes\"\nz.put(\"baseDir\", baseDir)\nprint(\"\"\"%html\n\u003cscript\u003e\n    var heads \u003d document.getElementsByTagName(\u0027h2\u0027);\n    var numHeads \u003d heads.length;\n    var inner \u003d \"\";\n    var i \u003d 0;\n    var j \u003d 0;\n    while (i \u003c numHeads){\n        inner \u003d heads[i].innerHTML;\n        if (inner.search(\".-\") !\u003d -1 ) {\n            j++;\n            heads[i].innerHTML \u003d inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n\u003c/script\u003e\n\"\"\")",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorHide": true,
        "title": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cscript\u003e\n    var heads \u003d document.getElementsByTagName(\u0027h2\u0027);\n    var numHeads \u003d heads.length;\n    var inner \u003d \"\";\n    var i \u003d 0;\n    var j \u003d 0;\n    while (i \u003c numHeads){\n        inner \u003d heads[i].innerHTML;\n        if (inner.search(\".-\") !\u003d -1 ) {\n            j++;\n            heads[i].innerHTML \u003d inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n\u003c/script\u003e\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1507319890492_174239002",
      "id": "20161011-125733_1279366716",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Oct 6, 2017 4:58:10 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1507319890493_173854253",
      "id": "20161011-131205_59629538",
      "dateCreated": "Oct 6, 2017 4:58:10 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Presentación 07 - Datasets DataFrames",
  "id": "2CU2YWF6S",
  "angularObjects": {
    "2CVGDCPRC:shared_process": [],
    "2CWU5CDHZ:shared_process": [],
    "2CT8824T5:shared_process": [],
    "2CU175KYA:shared_process": [],
    "2CVW8KG5T:shared_process": [],
    "2CVUG7AYZ:shared_process": [],
    "2CU2PGWJK:shared_process": [],
    "2CTY6NNQE:shared_process": [],
    "2CWN7MJ7T:shared_process": [],
    "2CU7V5RQS:shared_process": [],
    "2CTE4TQQ3:shared_process": [],
    "2CWGK27UU:shared_process": [],
    "2CT7JSWR8:shared_process": [],
    "2CWKBNSF7:shared_process": [],
    "2CWTDEFZ3:shared_process": [],
    "2CUBD912K:shared_process": [],
    "2CW4YQTDJ:shared_process": [],
    "2CTW8ZN43:shared_process": [],
    "2CVTSPTRQ:shared_process": []
  },
  "config": {},
  "info": {}
}