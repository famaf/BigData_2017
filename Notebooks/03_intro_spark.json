{"paragraphs":[{"text":"val baseDir=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/02_intro_spark/notebook\"\nprint(\"\"\"%html\n<center>\n    <h1>Programación Distribuida sobre Grandes Volúmenes de Datos</h1>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/comun/logo%20UNC%20FAMAF%202016.svg\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<h4 style=\"text-align:center;\"> Damián Barsotti  </h4>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n\"\"\")\n","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<center>\n    <h1>Programación Distribuida sobre Grandes Volúmenes de Datos</h1>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/comun/logo%20UNC%20FAMAF%202016.svg\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<h4 style=\"text-align:center;\"> Damián Barsotti  </h4>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n"}]},"apps":[],"jobName":"paragraph_1506481116126_-486050327","id":"20160720-131940_474698556","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:26"},{"text":"%md\n#Introducción a Spark\n","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Introducción a Spark</h1>\n"}]},"apps":[],"jobName":"paragraph_1506481116145_-494899551","id":"20160628-160644_98292392","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27"},{"text":"%md\nPara correr en Spark los siguientes ejemplos puede utilizar:\n\n* **Shell Spark** \n    - En una consola y desde el directorio base donde se descomprimió ejecutar `./bin/spark-shell`\n\n* **Notebook Zeppelin**\n    - Ejecutar `./bin/zeppelin.sh` en el directorio de Zeppelin\n    - En un navegador ir a [http://localhost:8080](http://localhost:8080)\n    - Troubleshooting:\n        - Probar agregar en ./conf/zeppelin-env.sh\n        ```\n        export JAVA_HOME=<directorio donde esta java 7 u 8>\n        ```\n        - Si quiere otra version de Spark, agregar en ./conf/zeppelin-env.sh\n        ```\n        export SPARK_HOME=<directorio donde esta Spark>\n        ```\n","dateUpdated":"2017-09-26T23:58:36-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Para correr en Spark los siguientes ejemplos puede utilizar:</p>\n<ul>\n  <li>\n    <p><strong>Shell Spark</strong></p>\n    <ul>\n      <li>En una consola y desde el directorio base donde se descomprimió ejecutar <code>./bin/spark-shell</code></li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Notebook Zeppelin</strong></p>\n    <ul>\n      <li>Ejecutar <code>./bin/zeppelin.sh</code> en el directorio de Zeppelin</li>\n      <li>En un navegador ir a <a href=\"http://localhost:8080\">http://localhost:8080</a></li>\n      <li>Troubleshooting:\n        <ul>\n          <li>\n          <p>Probar agregar en ./conf/zeppelin-env.sh</p>\n          <pre><code>export JAVA_HOME=&lt;directorio donde esta java 7 u 8&gt;\n</code></pre></li>\n          <li>\n          <p>Si quiere otra version de Spark, agregar en ./conf/zeppelin-env.sh</p>\n          <pre><code>export SPARK_HOME=&lt;directorio donde esta Spark&gt;\n</code></pre></li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1506481116145_-494899551","id":"20160621-130520_1525214395","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28"},{"title":"Para ver las versiones de Spark y Scala","text":"sc.version\nutil.Properties.versionNumberString\n","user":"anonymous","dateUpdated":"2017-09-27T00:11:20-0300","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres0: String = 2.1.0\n\nres1: String = 2.11.8\n"}]},"apps":[],"jobName":"paragraph_1506481116146_-493745305","id":"20170830-114757_1684133948","dateCreated":"2017-09-26T23:58:36-0300","dateStarted":"2017-09-27T00:11:21-0300","dateFinished":"2017-09-27T00:11:51-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:29"},{"text":"%md\n##Conceptos básicos de Spark (Core)\n","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Conceptos básicos de Spark (Core)</h2>\n"}]},"apps":[],"jobName":"paragraph_1506481116147_-494130053","id":"20160711-163659_370336078","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:30"},{"text":"%md\n### Driver\n\nToda aplicación Spark tiene un programa **driver**:\n* lanza las operaciones en el cluster,\n* contiene nuestro programa\n* que define datos distribuidos y les aplica operaciones.\n\n`spark-shell` es un *programa driver* que de forma interactiva ejecuta las operaciones que queremos correr.\n\n### Executors\n\nEl driver maneja y envía tareas a un número de nodos (o threads en modo local) llamados **executors**.\n","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Driver</h3>\n<p>Toda aplicación Spark tiene un programa <strong>driver</strong>:</p>\n<ul>\n<li>lanza las operaciones en el cluster,</li>\n<li>contiene nuestro programa</li>\n<li>que define datos distribuidos y les aplica operaciones.</li>\n</ul>\n<p><code>spark-shell</code> es un <em>programa driver</em> que de forma interactiva ejecuta las operaciones que queremos correr.</p>\n<h3>Executors</h3>\n<p>El driver maneja y envía tareas a un número de nodos (o threads en modo local) llamados <strong>executors</strong>.</p>\n"}]},"apps":[],"jobName":"paragraph_1506481116147_-494130053","id":"20160628-152626_1667111137","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:31"},{"text":"\nprintln(s\"\"\"%html\n<img src=\"$baseDir/driver_exec.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n\"\"\")","dateUpdated":"2017-09-26T23:58:36-0300","config":{"tableHide":false,"editorSetting":{},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/03_intro_spark_notebook/driver_exec.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n\n"}]},"apps":[],"jobName":"paragraph_1506481116148_-496053798","id":"20160628-160327_750179531","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32"},{"text":"%md\n###SparkContext","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>SparkContext</h3>\n"}]},"apps":[],"jobName":"paragraph_1506481116149_-496438547","id":"20160711-180525_841843918","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:33"},{"text":"%md &nbsp;\n\n* Los programas en el driver acceden a Spark a través de un objeto `SparkContext`\n* Le dice a Spark como conectarce con el cluster (o a los distintos threads en modo local)\n    - (representa la conección al cluster) \n* En el shell está predefinida la variable `sc` de tipo `SparkContext`\n    - (otros programas deben crearla con `new`)\n","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>&nbsp;</p>\n<ul>\n<li>Los programas en el driver acceden a Spark a través de un objeto <code>SparkContext</code></li>\n<li>Le dice a Spark como conectarce con el cluster (o a los distintos threads en modo local)<ul>\n<li>(representa la conección al cluster)</li>\n</ul>\n</li>\n<li>En el shell está predefinida la variable <code>sc</code> de tipo <code>SparkContext</code><ul>\n<li>(otros programas deben crearla con <code>new</code>)</li>\n</ul>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481116149_-496438547","id":"20160628-094604_1157091295","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:34"},{"text":"sc","user":"anonymous","dateUpdated":"2017-09-27T00:12:00-0300","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres3: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4b2bda7f\n"}]},"apps":[],"jobName":"paragraph_1506481116150_-495284300","id":"20160711-175152_1368822013","dateCreated":"2017-09-26T23:58:36-0300","dateStarted":"2017-09-27T00:12:00-0300","dateFinished":"2017-09-27T00:12:00-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:35"},{"text":"%md\n* `SparkContext` tiene el parámetro `master` que indica que cluster usar\n    - permite elegir cluster de forma transparente\n","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<ul>\n<li><code>SparkContext</code> tiene el parámetro <code>master</code> que indica que cluster usar<ul>\n<li>permite elegir cluster de forma transparente</li>\n</ul>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481116150_-495284300","id":"20160711-175305_229182674","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36"},{"text":"print(s\"\"\"%table\nmaster\\tdescripción\nlocal\\tSpark corre localmente con un solo worker (no paralelismo)\nlocal[K]\\tSpark corre localmente con K threads\nspark://HOST:PORT\\tse conecta a un cluster Spark\nmesos://HOST:PORT\\tse conecta a un cluster Mesos\n...\\t...\n\"\"\")\n","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":190,"optionOpen":true,"keys":[{"name":"master","index":0,"aggr":"sum"}],"values":[{"name":"descripción","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"master","index":0,"aggr":"sum"},"yAxis":{"name":"descripción","index":1,"aggr":"sum"}}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"master\tdescripción\nlocal\tSpark corre localmente con un solo worker (no paralelismo)\nlocal[K]\tSpark corre localmente con K threads\nspark://HOST:PORT\tse conecta a un cluster Spark\nmesos://HOST:PORT\tse conecta a un cluster Mesos\n...\t...\n"}]},"apps":[],"jobName":"paragraph_1506481116151_-495669049","id":"20160711-183252_1072432438","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37"},{"text":"sc.master","user":"anonymous","dateUpdated":"2017-09-27T00:12:06-0300","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres4: String = local[*]\n"}]},"apps":[],"jobName":"paragraph_1506481116151_-495669049","id":"20160711-185649_816555919","dateCreated":"2017-09-26T23:58:36-0300","dateStarted":"2017-09-27T00:12:06-0300","dateFinished":"2017-09-27T00:12:06-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38"},{"text":"print(s\"\"\"%html\n<img src=\"$baseDir/cluster-overview.png\" alt=\"Drawing\"/>\n\"\"\")","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":280,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/03_intro_spark_notebook/cluster-overview.png\" alt=\"Drawing\"/>\n"}]},"apps":[],"jobName":"paragraph_1506481116152_-497592794","id":"20160711-183612_1767738407","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:39"},{"text":"%md\n#### Tareas\n\n1. Se conecta con un *cluster mannager* el cual asigna los recursos en el cluster\n2. Adquiere *executors* en los nodos del cluster para correr aplicaciones y guardar datos\n3. Envía taréas a los *executors* para que las ejecuten","dateUpdated":"2017-09-26T23:58:36-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Tareas</h4>\n<ol>\n  <li>Se conecta con un <em>cluster mannager</em> el cual asigna los recursos en el cluster</li>\n  <li>Adquiere <em>executors</em> en los nodos del cluster para correr aplicaciones y guardar datos</li>\n  <li>Envía taréas a los <em>executors</em> para que las ejecuten</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1506481116153_-497977542","id":"20160713-194503_1960986369","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:40"},{"text":"%md\n### Introducción a Resilient Distributed Dataset (RDD)","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Introducción a Resilient Distributed Dataset (RDD)</h3>\n"}]},"apps":[],"jobName":"paragraph_1506481116153_-497977542","id":"20160711-190135_1948031726","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:41"},{"text":"%md\n* Son la abstracción base de Spark (existen otras API's)\n\n* Contenedores de objetos inmutables, distribuidos en el cluster (contiene los datos)\n\n* Creados con el `SparkContext`\n* Tipados estatico: `RDD[T]` tiene objetos de tipo `T`\n* Internamente se divide en **particiones** que pueden ser procesadas en distintos nodos\n    - partición y almacenamiento controlados por el usuario\n\n* Creados al cargar datasets o por transformaciones paralelas (`map`, `filter`, ...)\n\n* Ante fallas se reconstruyen (resilencia)\n* **Importante**: todo lo que no derive del `SparkContext` corre solo en el **driver**\n","dateUpdated":"2017-09-26T23:58:36-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>\n  <p>Son la abstracción base de Spark (existen otras API&rsquo;s)</p></li>\n  <li>\n  <p>Contenedores de objetos inmutables, distribuidos en el cluster (contiene los datos)</p></li>\n  <li>\n  <p>Creados con el <code>SparkContext</code></p></li>\n  <li>Tipados estatico: <code>RDD[T]</code> tiene objetos de tipo <code>T</code></li>\n  <li>Internamente se divide en <strong>particiones</strong> que pueden ser procesadas en distintos nodos\n    <ul>\n      <li>partición y almacenamiento controlados por el usuario</li>\n    </ul>\n  </li>\n  <li>\n  <p>Creados al cargar datasets o por transformaciones paralelas (<code>map</code>, <code>filter</code>, &hellip;)</p></li>\n  <li>\n  <p>Ante fallas se reconstruyen (resilencia)</p></li>\n  <li><strong>Importante</strong>: todo lo que no derive del <code>SparkContext</code> corre solo en el <strong>driver</strong></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1506481116154_-496823296","id":"20160712-154726_1373945036","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:42"},{"text":"val lines = sc.textFile(\"README.md\") // Crea un RDD que contiene \n                                     //cada linea\nlines.count() // Cuenta el número de elementos en este RDD\nlines.first() // Primer elemento del RDD, o sea la primera linea de README.md","user":"anonymous","dateUpdated":"2017-09-27T00:12:10-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:27\n\nres6: Long = 29\n\nres7: String = # Apache Zeppelin\n"}]},"apps":[],"jobName":"paragraph_1506481116155_-497208045","id":"20160621-131810_1623359813","dateCreated":"2017-09-26T23:58:36-0300","dateStarted":"2017-09-27T00:12:10-0300","dateFinished":"2017-09-27T00:12:13-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:43"},{"text":"val lines = sc.textFile(\"README.md\") // Crea el RDD\nval sparkLines = lines.filter(_.contains(\"Spark\")) // Filtra en paralelo crando otro RDD\nvar l = sparkLines.first() // Primer elemento del RDD, o sea la primera linea de README.md\nl = l.replace(\"Spark\",\"Sparkling\") // Esto corre en el driver","user":"anonymous","dateUpdated":"2017-09-27T00:12:17-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[3] at textFile at <console>:27\n\nsparkLines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at <console>:29\n\nl: String = \"   * Built-in Apache Spark support\"\n\nl: String =    * Built-in Apache Sparkling support\n"}]},"apps":[],"jobName":"paragraph_1506481116155_-497208045","id":"20160711-164422_1273092263","dateCreated":"2017-09-26T23:58:36-0300","dateStarted":"2017-09-27T00:12:17-0300","dateFinished":"2017-09-27T00:12:20-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:44"},{"text":"%md\n* Evaluados de forma **lazy** (otra idea funcional)\n\n* Operaciones evaluadas lazy + tipos estático:\n\n    - Estrategias automáticas de optimización\n    - Detección de errores antes que corran en el cluster\n    ","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<ul>\n<li><p>Evaluados de forma <strong>lazy</strong> (otra idea funcional)</p>\n</li>\n<li><p>Operaciones evaluadas lazy + tipos estático:</p>\n<ul>\n<li>Estrategias automáticas de optimización</li>\n<li>Detección de errores antes que corran en el cluster</li>\n</ul>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481116156_-499131789","id":"20160712-163342_531450870","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:45"},{"text":"val lines = sc.textFile(\"logs/*\")\n\nval errors = lines.filter(_.startsWith(\"ERROR\"))\n//errors.collect\nval messages = errors.map(_.split(' ')(1))\n// Hasta aquí se evalúa de form lazy\n\nmessages.collect()(1)\n// Se envía la computación","user":"anonymous","dateUpdated":"2017-09-27T00:12:32-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlines: org.apache.spark.rdd.RDD[String] = logs/* MapPartitionsRDD[6] at textFile at <console>:27\n\nerrors: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[7] at filter at <console>:30\n\nmessages: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at map at <console>:32\n\nres12: String = [2017-08-21\n"}]},"apps":[],"jobName":"paragraph_1506481116156_-499131789","id":"20160711-173159_582518652","dateCreated":"2017-09-26T23:58:36-0300","dateStarted":"2017-09-27T00:12:32-0300","dateFinished":"2017-09-27T00:12:36-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:46"},{"title":"FIN","text":"val baseDir=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/03_intro_spark_notebook\"\nprintln(\"\"\"%html\n<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\"~\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(\"~\",\"\"+j);\n        }\n        i++\n    }\n</script>\n\"\"\")\n","dateUpdated":"2017-09-26T23:58:36-0300","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1506481116157_-499516538","id":"20160712-175904_2058049512","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:47"},{"dateUpdated":"2017-09-26T23:58:36-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1506481116158_-498362291","id":"20160829-174645_1462733036","dateCreated":"2017-09-26T23:58:36-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:48"}],"name":"Presentación 02 - Introducción a Spark","id":"2CWEYUW32","angularObjects":{"2CQY683MD:shared_process":[],"2CRGPDJTN:shared_process":[],"2CTZD3XCX:shared_process":[],"2CRMCDEQM:shared_process":[],"2CTAV9E28:shared_process":[],"2CRA7KCDA:shared_process":[],"2CQ81X7G5:shared_process":[],"2CRUQ6EVN:shared_process":[],"2CTHYC7X9:shared_process":[],"2CRHWDFYM:shared_process":[],"2CQJ7MXW5:shared_process":[],"2CQAR5VSM:shared_process":[],"2CRCFF6HB:shared_process":[],"2CTK8TR8Q:shared_process":[],"2CQN5Q87B:shared_process":[],"2CTW5YZGW:shared_process":[],"2CSR7EJVT:shared_process":[],"2CRUQ7EXT:shared_process":[],"2CRFFBRXJ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}