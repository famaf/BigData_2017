{"paragraphs":[{"text":"print(\"\"\"%html\n<center>\n    <h1>Programación Distribuida sobre Grandes Volúmenes de Datos</h1>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/comun/logo%20UNC%20FAMAF%202016.svg\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<h4 style=\"text-align:center;\"> Damián Barsotti  </h4>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n\"\"\")\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<center>\n    <h1>Programación Distribuida sobre Grandes Volúmenes de Datos</h1>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/comun/logo%20UNC%20FAMAF%202016.svg\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<h4 style=\"text-align:center;\"> Damián Barsotti  </h4>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n"}]},"apps":[],"jobName":"paragraph_1506481120749_-1135491146","id":"20160719-195708_121333589","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:63"},{"text":"%md\n#Resilient Distributed Dataset (RDD)\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Resilient Distributed Dataset (RDD)</h1>\n"}]},"apps":[],"jobName":"paragraph_1506481120752_-1124333428","id":"20160713-164104_891230593","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"%md\n## 3.- Creación\n\nDos maneras:\n1. Cargando datos externos\n1. Paralelizando un contenedor\n    - método `paralelize`\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>3.- Creación</h2>\n<p>Dos maneras:</p>\n<ol>\n<li>Cargando datos externos</li>\n<li>Paralelizando un contenedor<ul>\n<li>método <code>paralelize</code></li>\n</ul>\n</li>\n</ol>\n"}]},"apps":[],"jobName":"paragraph_1506481120752_-1124333428","id":"20160713-164122_1281043853","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"text":"val lines = sc.textFile(\"README.md\")","user":"anonymous","dateUpdated":"2017-09-27T00:13:03-0300","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[10] at textFile at <console>:27\n"}]},"apps":[],"jobName":"paragraph_1506481120753_-1124718177","id":"20160713-171236_6569646","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:13:03-0300","dateFinished":"2017-09-27T00:13:03-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"text":"val lines = sc.parallelize(List(\"Hola che.\", \"que tal\"))","user":"anonymous","dateUpdated":"2017-09-27T00:13:08-0300","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlines: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[11] at parallelize at <console>:27\n"}]},"apps":[],"jobName":"paragraph_1506481120754_-1123563930","id":"20160713-171137_1733510384","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:13:08-0300","dateFinished":"2017-09-27T00:13:09-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"text":"%md\nSpark divide el dataset en múltiples **particiones** que pueden ser operadas en distintos nodos del cluster:","dateUpdated":"2017-09-26T23:58:40-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Spark divide el dataset en múltiples <strong>particiones</strong> que pueden ser operadas en distintos nodos del cluster:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1506481120757_-1126257172","id":"20160713-170635_1564475007","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:68"},{"text":"val data = 1 to 50\nval distData = sc.parallelize(data,10) // 10 indica la cantidad de particiones, probar tambien sin.\ndistData.partitions.size","user":"anonymous","dateUpdated":"2017-09-27T00:13:21-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndata: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50)\n\ndistData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at <console>:29\n\nres15: Int = 10\n"}]},"apps":[],"jobName":"paragraph_1506481120757_-1126257172","id":"20160715-123052_1819777592","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:13:21-0300","dateFinished":"2017-09-27T00:13:23-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"text":"%md\n## ~.- Operaciones sobre RDD's\n\nDos clases de operaciones:\n* Transformaciones\n* Acciones","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.- Operaciones sobre RDD's</h2>\n<p>Dos clases de operaciones:</p>\n<ul>\n<li>Transformaciones</li>\n<li>Acciones</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481120758_-1125102925","id":"20160713-171507_931528469","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:70"},{"text":"%md\n### Transformaciones\n\n* Devuelven un nuevo RDD transformando el original\n* Son lazy (se acumulan y se ejecutan cuando se necesite un resultado)\n\n\nVeamos el siguiente ejemplo:\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Transformaciones</h3>\n<ul>\n<li>Devuelven un nuevo RDD transformando el original</li>\n<li>Son lazy (se acumulan y se ejecutan cuando se necesite un resultado)</li>\n</ul>\n<p>Veamos el siguiente ejemplo:</p>\n"}]},"apps":[],"jobName":"paragraph_1506481120758_-1125102925","id":"20160713-171630_1976079115","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:71"},{"text":"val inputRDD = sc.textFile(\"./doc/log.txt\") // RDD. El archivo log.txt está en el aula virtual.\nval errorRDD = inputRDD.filter(line => line.contains(\"ERROR\")) // se crea un nuevo RDD","user":"anonymous","dateUpdated":"2017-09-27T00:13:28-0300","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ninputRDD: org.apache.spark.rdd.RDD[String] = ./doc/log.txt MapPartitionsRDD[14] at textFile at <console>:27\n\nerrorRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[15] at filter at <console>:29\n"}]},"apps":[],"jobName":"paragraph_1506481120759_-1125487674","id":"20160713-171804_626161146","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:13:28-0300","dateFinished":"2017-09-27T00:13:29-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:72"},{"text":"%md &nbsp;\nNotar que `filter` no muta el RDD `inputRDD` si no que crea uno nuevo (los RDD son inmutables).\nCon lo cual podríamos volver a utilizar inputRDD:","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>&nbsp;\n<br  />Notar que <code>filter</code> no muta el RDD <code>inputRDD</code> si no que crea uno nuevo (los RDD son inmutables).\n<br  />Con lo cual podríamos volver a utilizar inputRDD:</p>\n"}]},"apps":[],"jobName":"paragraph_1506481120760_-1127411419","id":"20160713-173758_874891123","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:73"},{"title":"Programa","text":"val inputRDD = sc.textFile(\"./doc/log.txt\") // RDD\nval errorRDD = inputRDD.filter(line => line.contains(\"ERROR\")) // se crea un nuevo RDD\nval configRDD = inputRDD.filter(line => line.contains(\"config\")) // se crea un nuevo RDD\n\nval errOrConfRDD = errorRDD.union(configRDD) \n//errOrConfRDD.toDebugString","user":"anonymous","dateUpdated":"2017-09-27T00:13:33-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ninputRDD: org.apache.spark.rdd.RDD[String] = ./doc/log.txt MapPartitionsRDD[17] at textFile at <console>:27\n\nerrorRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[18] at filter at <console>:29\n\nconfigRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[19] at filter at <console>:29\n\nerrOrConfRDD: org.apache.spark.rdd.RDD[String] = UnionRDD[20] at union at <console>:34\n"}]},"apps":[],"jobName":"paragraph_1506481120760_-1127411419","id":"20160713-174309_1524556660","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:13:33-0300","dateFinished":"2017-09-27T00:13:35-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:74"},{"text":"%md\nComo se ve en la linea 5 las transformaciones pueden operar sobre uno o más RDD's.\nLa transformación `union` crea un nuevo RDD con la lineas que contienen tanto la palabra \"ERROR\" como \"config\"\n(una forma mejor sería filtrar las lineas con \"ERROR\" y \"config\" al mismo tiempo).\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Como se ve en la linea 5 las transformaciones pueden operar sobre uno o más RDD's.\n<br  />La transformación <code>union</code> crea un nuevo RDD con la lineas que contienen tanto la palabra &ldquo;ERROR&rdquo; como &ldquo;config&rdquo;\n<br  />(una forma mejor sería filtrar las lineas con &ldquo;ERROR&rdquo; y &ldquo;config&rdquo; al mismo tiempo).</p>\n"}]},"apps":[],"jobName":"paragraph_1506481120761_-1127796168","id":"20160713-174954_1060101920","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:75"},{"text":"%md &nbsp;\n\nSpark hace un seguimiento de las dependencias entre RDD's llamadas **grafo de dependencias** (linage graph):","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>&nbsp;</p>\n<p>Spark hace un seguimiento de las dependencias entre RDD's llamadas <strong>grafo de dependencias</strong> (linage graph):</p>\n"}]},"apps":[],"jobName":"paragraph_1506481120762_-1126641921","id":"20160713-190711_1338428430","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:76"},{"title":"Grafo de dependencias","text":"print(s\"\"\"%html\n<img src=\"$baseDir/log_linage.png\" alt=\"Drawing\" style=\"width: 70%;\"/>\n\"\"\")","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/04_rdd_notebook/log_linage.png\" alt=\"Drawing\" style=\"width: 70%;\"/>\n"}]},"apps":[],"jobName":"paragraph_1506481120763_-1127026670","id":"20160713-175804_440530333","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:77"},{"title":"Algunas Transformaciones...rdd={1,2,3,3}","text":"print(s\"\"\"\n%table\nTransformación\\tDescripción\\t Ejemplo \\t Resultado\n%html <b>map</b>(func)\\t Devuelve un nuevo rdd transformando cada elemento con la función func. \\t rdd.map(x => x+1) \\t {2,3,4,4}\n%html <b>filter</b>(func)\\t Devuelve un nuevo RDD seleccionando los elementos con la función func. \\t rdd.filter(x => x!=1) \\t {2,3,3}\n%html <b>flatMap</b>(func)\\t Similar a map pero cada elemento es mapeado a un Seq en vez de a un elemento y los concatena. \\t rdd.flatMap(x=>x.to(3)) \\t {1,2,3,2,3,3,3}\n%html <b>distinct</b>() \\t Remueve duplicados.\\t rdd.distinct() \\t {1,2,3}\n%html <b>sample</b>(con_reemplazo,frac,[seed])\\t Toma una muestra del RDD.  \\t rdd.sample(false,0.5) \\t No determinista \n\\t Sin reemplazo: frac es probabilidad de elección de un elemento.\\t \\t\n\\t Con reemplazo: numero de veces esperado que un elemento es elegido (>=0).\\t \\t\n\"\"\")","dateUpdated":"2017-09-26T23:58:40-0300","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":224,"optionOpen":false,"keys":[{"name":"Transformación","index":0,"aggr":"sum"}],"values":[{"name":"Descripción","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Transformación","index":0,"aggr":"sum"},"yAxis":{"name":"Descripción","index":1,"aggr":"sum"}}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"TABLE","data":"Transformación\tDescripción\t Ejemplo \t Resultado\n%html <b>map</b>(func)\t Devuelve un nuevo rdd transformando cada elemento con la función func. \t rdd.map(x => x+1) \t {2,3,4,4}\n%html <b>filter</b>(func)\t Devuelve un nuevo RDD seleccionando los elementos con la función func. \t rdd.filter(x => x!=1) \t {2,3,3}\n%html <b>flatMap</b>(func)\t Similar a map pero cada elemento es mapeado a un Seq en vez de a un elemento y los concatena. \t rdd.flatMap(x=>x.to(3)) \t {1,2,3,2,3,3,3}\n%html <b>distinct</b>() \t Remueve duplicados.\t rdd.distinct() \t {1,2,3}\n%html <b>sample</b>(con_reemplazo,frac,[seed])\t Toma una muestra del RDD.  \t rdd.sample(false,0.5) \t No determinista \n\t Sin reemplazo: frac es probabilidad de elección de un elemento.\t \t\n\t Con reemplazo: numero de veces esperado que un elemento es elegido (>=0).\t \t\n"}]},"apps":[],"jobName":"paragraph_1506481120763_-1127026670","id":"20170830-121826_376153165","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:78"},{"title":"Transformaciones 2 RDD's...rdd={1,2,3},other={3,4,5}","text":"print(s\"\"\"\n%table\nTransformación\\tdescripción\\t Ejemplo \\t Resultado\n%html <b>union</b>(otroRDD)\\t Devuelve la unión del RDD y el argumento con repeticiones. \\t rdd.union(other) \\t {1,2,3,3,4,5}\n%html <b>intersection</b>(otroRDD)\\t Devuelve los elementos que estan en el RDD y el argumento. \\t rdd.intersection(other) \\t {3}\n%html <b>subtract</b>(otroRDD)\\t Devuelve la resta del RDD y el argumento. \\t rdd.substract(other) \\t {1,2}\n%html <b>cartesian</b>(otroRDD)\\t Devuelve el producto cartesiano. \\t rdd.cartesian(other) \\t {(1,3),(1,4),(1,5),(2,3),...,(3,5)}\n\"\"\")\n\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":184,"optionOpen":false,"keys":[{"name":"Transformación","index":0,"aggr":"sum"}],"values":[{"name":"descripción","index":1,"aggr":"sum"}],"groups":[],"scatter":{"yAxis":{"name":"descripción","index":1,"aggr":"sum"}}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"TABLE","data":"Transformación\tdescripción\t Ejemplo \t Resultado\n%html <b>union</b>(otroRDD)\t Devuelve la unión del RDD y el argumento con repeticiones. \t rdd.union(other) \t {1,2,3,3,4,5}\n%html <b>intersection</b>(otroRDD)\t Devuelve los elementos que estan en el RDD y el argumento. \t rdd.intersection(other) \t {3}\n%html <b>subtract</b>(otroRDD)\t Devuelve la resta del RDD y el argumento. \t rdd.substract(other) \t {1,2}\n%html <b>cartesian</b>(otroRDD)\t Devuelve el producto cartesiano. \t rdd.cartesian(other) \t {(1,3),(1,4),(1,5),(2,3),...,(3,5)}\n"}]},"apps":[],"jobName":"paragraph_1506481120764_-1128950414","id":"20160718-203427_949028664","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:79"},{"title":"Notas:","text":"%md\n* Las transformaciones no son sobre conjuntos ya que los RDD's tienen duplicados (son *multiconjuntos*).\n* `intersection` remueve duplicados en el resultado.\n* `distinct`, `intersection`, `substract` y `cartesian` son poco eficientes ya que requiere transferencia de datos.\n\nPara más información y transformaciones ver:\n* [Spark RDD Api Guide](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD).\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<ul>\n<li>Las transformaciones no son sobre conjuntos ya que los RDD's tienen duplicados (son <em>multiconjuntos</em>).</li>\n<li><code>intersection</code> remueve duplicados en el resultado.</li>\n<li><code>distinct</code>, <code>intersection</code>, <code>substract</code> y <code>cartesian</code> son poco eficientes ya que requiere transferencia de datos.</li>\n</ul>\n<p>Para más información y transformaciones ver:</p>\n<ul>\n<li><a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD\">Spark RDD Api Guide</a>.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481120764_-1128950414","id":"20160718-215211_1535950499","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:80"},{"text":"%md\n### Acciones\n\n* Devuelven un resultado final al *driver*, o\n* escriben datos en almacenamiento externo.\n* Fuerzan la evaluación de las transformaciones.\n\nSiguiendo el ejemplo anterior, contemos la cantidad de lineas (con la acción `count`) y mostremos los primeros 10 resultados (acción `take`): ","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Acciones</h3>\n<ul>\n<li>Devuelven un resultado final al <em>driver</em>, o</li>\n<li>escriben datos en almacenamiento externo.</li>\n<li>Fuerzan la evaluación de las transformaciones.</li>\n</ul>\n<p>Siguiendo el ejemplo anterior, contemos la cantidad de lineas (con la acción <code>count</code>) y mostremos los primeros 10 resultados (acción <code>take</code>):</p>\n"}]},"apps":[],"jobName":"paragraph_1506481120764_-1128950414","id":"20160713-194134_1567089309","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:81"},{"title":"programa","text":"val inputRDD = sc.textFile(\"./doc/log.txt\") // En aula virtual \"Archivos de prueba\" \nval errorRDD = inputRDD.filter(line => line.contains(\"ERROR\")) // se crea un nuevo RDD\nval configRDD = inputRDD.filter(line => line.contains(\"config\")) // se crea un nuevo RDD\nval errOrConfRDD = errorRDD.union(configRDD) \n\n//Acciones:\nprintln(\"El archivo tiene \"+ errOrConfRDD.count + \"\"\" lineas que contienen \"error\" o \"config\".\"\"\")\nprintln(\"Las primeras 10 son:\")\nerrOrConfRDD.take(10).foreach(println) //","user":"anonymous","dateUpdated":"2017-09-27T00:13:51-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":7,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ninputRDD: org.apache.spark.rdd.RDD[String] = ./doc/log.txt MapPartitionsRDD[22] at textFile at <console>:27\n\nerrorRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[23] at filter at <console>:29\n\nconfigRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[24] at filter at <console>:29\n\nerrOrConfRDD: org.apache.spark.rdd.RDD[String] = UnionRDD[25] at union at <console>:33\nEl archivo tiene 520 lineas que contienen \"error\" o \"config\".\nLas primeras 10 son:\n\t(WARN) warning, (ERROR) error, (NI) not implemented, (??) unknown.\n[  5368.677] (ERROR) evdev: TOSHIBA Web Camera: Unable to open evdev device \"/dev/input/event5\".\n[  5766.253] (ERROR) evdev: TOSHIBA Web Camera: Unable to open evdev device \"/dev/input/event5\".\n[ 19035.278] (ERROR) evdev: TOSHIBA Web Camera: Unable to open evdev device \"/dev/input/event5\".\n[ 21216.883] (ERROR) evdev: TOSHIBA Web Camera: Unable to open evdev device \"/dev/input/event5\".\n[ 21527.723] (ERROR) evdev: TOSHIBA Web Camera: Unable to open evdev device \"/dev/input/event5\".\n[ 22341.291] (ERROR) evdev: TOSHIBA Web Camera: Unable to open evdev device \"/dev/input/event5\".\n[ 25750.790] (ERROR) evdev: TOSHIBA Web Camera: Unable to open evdev device \"/dev/input/event5\".\n[ 28424.566] (ERROR) evdev: TOSHIBA Web Camera: Unable to open evdev device \"/dev/input/event5\".\n[ 29784.843] (ERROR) evdev: TOSHIBA Web Camera: Unable to open evdev device \"/dev/input/event5\".\n"}]},"apps":[],"jobName":"paragraph_1506481120765_-1129335163","id":"20160713-200021_1787135967","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:13:51-0300","dateFinished":"2017-09-27T00:13:54-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:82"},{"title":"Spark UI","text":"{\nval uiHost = sc.getConf.getOption(\"spark.driver.host\").getOrElse(\"localhost\")\nval uiPort = sc.getConf.getOption(\"spark.ui.port\").getOrElse(\"4040\")\nprint(s\"\"\"\n%html\nVer resultado en Spark UI\n<a href=\"http://$uiHost:$uiPort\">http://$uiHost(host):$uiPort(port)</a>.\n<br>\n(ejecutar antes esta celda para detectar link)\n<br>\nNota: Total Tasks = numero de particiones\n<br>\nVer tambien Dag Visualization\n\"\"\")\n}","dateUpdated":"2017-09-26T23:58:40-0300","config":{"editorSetting":{"language":"scala"},"colWidth":5,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"HTML","data":"Ver resultado en Spark UI\n<a href=\"http://200.16.17.38:4040\">http://200.16.17.38(host):4040(port)</a>.\n<br>\n(ejecutar antes esta celda para detectar link)\n<br>\nNota: Total Tasks = numero de particiones\n<br>\nVer tambien Dag Visualization\n"}]},"apps":[],"jobName":"paragraph_1506481120765_-1129335163","id":"20160715-152919_1733799639","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:83"},{"text":"%md &nbsp;\n\n* Para devolver el contenido de todo un RDD se puede utilizar la acción `collect`, aunque no es recomendado para datasets muy grandes. En estos casos es conveniente guardar el resultado en algún file system (lo veremos más adelante).\n    -  Ver ejemplo anterior cuantas tareas si uso collect en vez de take.\n    -  Que tiene que ver con la evaluación lazy?\n* Se puede ver el grafo de dependencias desde el shell con `rdd.toDebugString`.","dateUpdated":"2017-09-26T23:58:40-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&nbsp;</p>\n<ul>\n  <li>Para devolver el contenido de todo un RDD se puede utilizar la acción <code>collect</code>, aunque no es recomendado para datasets muy grandes. En estos casos es conveniente guardar el resultado en algún file system (lo veremos más adelante).\n    <ul>\n      <li>Ver ejemplo anterior cuantas tareas si uso collect en vez de take.</li>\n      <li>Que tiene que ver con la evaluación lazy?</li>\n    </ul>\n  </li>\n  <li>Se puede ver el grafo de dependencias desde el shell con <code>rdd.toDebugString</code>.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1506481120765_-1129335163","id":"20160714-065200_1376556822","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:84"},{"title":"Reduce","text":"%md\nLa acción `reduce()` toma un operador asociativo y conmutativo. \nEjemplo:\n```scala\nval minimo = rdd.reduce((x,y)=> x min y)\n```\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>La acción <code>reduce()</code> toma un operador asociativo y conmutativo.\n<br  />Ejemplo:</p>\n<pre><code class=\"scala\">val minimo = rdd.reduce((x,y)=&gt; x min y)\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1506481120766_-1128180917","id":"20160719-130149_1926671124","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85"},{"title":"fold","text":"%md\nLa acción `fold()` toma un operador asociativo, conmutativo y su neutro (monoide conmutativo). \nEjemplo:\n```scala\nval prod = rdd.fold(1)(_ * _)\n```\nVentaja con `reduce`: \n* si el RDD almacena objetos mutables se puede minimizar la creación devolviendo el primer parámetro mutado.","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>La acción <code>fold()</code> toma un operador asociativo, conmutativo y su neutro (monoide conmutativo).\n<br  />Ejemplo:</p>\n<pre><code class=\"scala\">val prod = rdd.fold(1)(_ * _)\n</code></pre>\n<p>Ventaja con <code>reduce</code>:</p>\n<ul>\n<li>si el RDD almacena objetos mutables se puede minimizar la creación devolviendo el primer parámetro mutado.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481120768_-1438288530","id":"20160719-130746_1301430051","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:86"},{"title":"agregate","text":"%md\n```\nagregate(<neutro>)(<seqOp>,<combOp>)\n```\n* Hace un `fold` pero puede devolver un tipo distinto a los elementos del RDD.\n* `<seqOP>` operador que acumula los resultados dentro de una partición (nodo). Puede devolver un tipo distinto.\n* `<combOp>` operador asociativo que combina los resultados de los acumuladores de cada partición.\n* `<neutro>` de `combOp`. \n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code>agregate(&lt;neutro&gt;)(&lt;seqOp&gt;,&lt;combOp&gt;)\n</code></pre>\n<ul>\n<li>Hace un <code>fold</code> pero puede devolver un tipo distinto a los elementos del RDD.</li>\n<li><code>&lt;seqOP&gt;</code> operador que acumula los resultados dentro de una partición (nodo). Puede devolver un tipo distinto.</li>\n<li><code>&lt;combOp&gt;</code> operador asociativo que combina los resultados de los acumuladores de cada partición.</li>\n<li><code>&lt;neutro&gt;</code> de <code>combOp</code>.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481120771_-1437903781","id":"20160719-135625_75076484","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:87"},{"title":"Programa (Hay Cero)","text":"val randoms = Seq.fill(10)(scala.util.Random.nextInt(10))\nval rdd = sc.parallelize(randoms)\nval hay0 : Boolean = rdd.aggregate(false)(\n                                  (acc,value) => acc || value == 0, \n                                  (acc1,acc2) => acc1 || acc2)","user":"anonymous","dateUpdated":"2017-09-27T00:14:12-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrandoms: Seq[Int] = List(4, 9, 7, 6, 1, 5, 7, 8, 5, 2)\n\nrdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at <console>:29\n\nhay0: Boolean = false\n"}]},"apps":[],"jobName":"paragraph_1506481120772_-1439827526","id":"20160719-141504_1398510666","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:14:12-0300","dateFinished":"2017-09-27T00:14:14-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:88"},{"title":"Programa (Promedio)","text":"val input = sc.parallelize(1 to 30)\nval result = input.aggregate((0, 0))(\n                             (acc, value) => (acc._1 + value, acc._2 + 1),\n                             (acc1, acc2) => \n                                  (acc1._1 + acc2._1, acc1._2 + acc2._2))\nval avg = result._1 / result._2.toDouble","user":"anonymous","dateUpdated":"2017-09-27T00:14:20-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ninput: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[27] at parallelize at <console>:27\n\nresult: (Int, Int) = (465,30)\n\navg: Double = 15.5\n"}]},"apps":[],"jobName":"paragraph_1506481120772_-1439827526","id":"20160719-143830_796010885","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:14:20-0300","dateFinished":"2017-09-27T00:14:22-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:89"},{"title":"Más Acciones...rdd={1,2,3,3}","text":"print(s\"\"\"\n%table\nAcción\\tDescripción\\t Ejemplo \\t Resultado\n%html <b>collect</b>()\\t Devuelve un array con todos los elementos del RDD. \\t rdd.collect(other) \\t Array(1,2,3,3)\n%html <b>take</b>(n)\\t Devuelve un array con n elementos. \\t rdd.take(2) \\t Array(1,3)\n%html <b>top</b>(n)\\t Devuelve un array con los n mayores elementos. \\t rdd.top(2) \\t Array(3,3)\n%html <b>takeOrdered</b>(n)\\t Devuelve un array con los n menores elementos. \\t rdd.takeOrdered(2) \\t Array(1,2)\n%html <b>takeSample</b>(con_reemplazo,n,[seed])\\t Devuelve n elementos al azar, con o sin reeemplazo. \\t rdd.takeSample(false,2) \\t No determinista\n%html <b>count</b>()\\t Cantidad de elementos. \\t rdd.count() \\t 4\n%html <b>countByValue</b>()\\t Map con la cantidad de veces que aparece cada elemento. \\t rdd.countByValue() \\t Map(1->1, 2->1, 3->2)\n%html <b>mean</b>()\\t Computa el promedio. \\t rdd.mean() \\t 3.8\n%html <b>variance</b>()\\t Computa la varianza. \\t rdd.variance() \\t 8.959999999999999\n%html <b>foreach</b>(func)\\t Aplica función sin devolver nada. \\t rdd.foreach(print) \\t Nada (pero imprime \"1233\")\n%html <b>saveAsTextFile</b>(outputFile)\\t Guarda el resultado en file system. \\t rdd.saveAsTextFile(\"archivo\") \\t Nada\n\"\"\")\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":368,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":383,"optionOpen":false,"keys":[{"name":"Acción","index":0,"aggr":"sum"}],"values":[{"name":"Descripción","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Acción","index":0,"aggr":"sum"},"yAxis":{"name":"Descripción","index":1,"aggr":"sum"}}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"TABLE","data":"Acción\tDescripción\t Ejemplo \t Resultado\n%html <b>collect</b>()\t Devuelve un array con todos los elementos del RDD. \t rdd.collect(other) \t Array(1,2,3,3)\n%html <b>take</b>(n)\t Devuelve un array con n elementos. \t rdd.take(2) \t Array(1,3)\n%html <b>top</b>(n)\t Devuelve un array con los n mayores elementos. \t rdd.top(2) \t Array(3,3)\n%html <b>takeOrdered</b>(n)\t Devuelve un array con los n menores elementos. \t rdd.takeOrdered(2) \t Array(1,2)\n%html <b>takeSample</b>(con_reemplazo,n,[seed])\t Devuelve n elementos al azar, con o sin reeemplazo. \t rdd.takeSample(false,2) \t No determinista\n%html <b>count</b>()\t Cantidad de elementos. \t rdd.count() \t 4\n%html <b>countByValue</b>()\t Map con la cantidad de veces que aparece cada elemento. \t rdd.countByValue() \t Map(1->1, 2->1, 3->2)\n%html <b>mean</b>()\t Computa el promedio. \t rdd.mean() \t 3.8\n%html <b>variance</b>()\t Computa la varianza. \t rdd.variance() \t 8.959999999999999\n%html <b>foreach</b>(func)\t Aplica función sin devolver nada. \t rdd.foreach(print) \t Nada (pero imprime \"1233\")\n%html <b>saveAsTextFile</b>(outputFile)\t Guarda el resultado en file system. \t rdd.saveAsTextFile(\"archivo\") \t Nada\n"}]},"apps":[],"jobName":"paragraph_1506481120773_-1440212275","id":"20160719-144149_776727846","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:90"},{"title":"Nota:","text":"%md\n* `take`, `top` y `takeOrdered` pueden tener un parámetro extra que defina el orden.\n* `saveAsTextFile` guarda el resultado en un directorio con el nombre `outputFile` (un archivo por partición).\n\nPara más información y acciones ver:\n* [Spark RDD Api Guide](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD).\n* Las acciones como `mean` y `variance` que operan sobre tipo `Double` aparecen en [Spark RDD Double Api Guide](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.DoubleRDDFunctions).\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<ul>\n<li><code>take</code>, <code>top</code> y <code>takeOrdered</code> pueden tener un parámetro extra que defina el orden.</li>\n<li><code>saveAsTextFile</code> guarda el resultado en un directorio con el nombre <code>outputFile</code> (un archivo por partición).</li>\n</ul>\n<p>Para más información y acciones ver:</p>\n<ul>\n<li><a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD\">Spark RDD Api Guide</a>.</li>\n<li>Las acciones como <code>mean</code> y <code>variance</code> que operan sobre tipo <code>Double</code> aparecen en <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.DoubleRDDFunctions\">Spark RDD Double Api Guide</a>.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481120773_-1440212275","id":"20160719-185841_1789126413","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:91"},{"text":"%md\n## ~.- Evaluación Lazy\n\nEn Spark todas las **transformaciones** son evaluadas de forma lazy:\n* son acumuladas como *grafo de dependencias*\n* se ejecutan al momento de realizar una acción según sus dependencias.\n\nEsto permite:\n* recalcular las dependencias si hay algún fallo (**resilencia**)\n* hacer **optimizaciones**\n    - se hace un *pipeling* de transformaciones sin guardar resultados intermedios \n    - se computa solo lo que hace falta (tiene mucho sentido en Big Data)\n\nTambién la carga de datos es lazy (ver ejemplos).","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.- Evaluación Lazy</h2>\n<p>En Spark todas las <strong>transformaciones</strong> son evaluadas de forma lazy:</p>\n<ul>\n<li>son acumuladas como <em>grafo de dependencias</em></li>\n<li>se ejecutan al momento de realizar una acción según sus dependencias.</li>\n</ul>\n<p>Esto permite:</p>\n<ul>\n<li>recalcular las dependencias si hay algún fallo (<strong>resilencia</strong>)</li>\n<li>hacer <strong>optimizaciones</strong><ul>\n<li>se hace un <em>pipeling</em> de transformaciones sin guardar resultados intermedios</li>\n<li>se computa solo lo que hace falta (tiene mucho sentido en Big Data)</li>\n</ul>\n</li>\n</ul>\n<p>También la carga de datos es lazy (ver ejemplos).</p>\n"}]},"apps":[],"jobName":"paragraph_1506481120774_-1439058028","id":"20160714-070728_716418389","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:92"},{"title":"Computa solo lo necesario:","text":"val distFile = sc.textFile(\"README.md\")\nval wordsFile = distFile.map(l => l.split(\" \"))\nwordsFile.first()(0) // se lee y aplica map solo a la primer linea","user":"anonymous","dateUpdated":"2017-09-27T00:14:43-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndistFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[29] at textFile at <console>:27\n\nwordsFile: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[30] at map at <console>:35\n\nres24: String = #\n"}]},"apps":[],"jobName":"paragraph_1506481120774_-1439058028","id":"20160714-071909_1557855172","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:14:43-0300","dateFinished":"2017-09-27T00:14:44-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:93"},{"title":"Lee solo lo necesario:","text":"val distFile = sc.textFile(\"README.md\")\nval zeppLines = distFile.filter(l => l.contains(\"Zeppelin\"))\nzeppLines.first // lee hasta la primer linea que tiene la palabra","user":"anonymous","dateUpdated":"2017-09-27T00:14:47-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndistFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[32] at textFile at <console>:27\n\nzeppLines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[33] at filter at <console>:35\n\nres25: String = # Apache Zeppelin\n"}]},"apps":[],"jobName":"paragraph_1506481120775_-1439442777","id":"20160714-072744_1911966320","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:14:47-0300","dateFinished":"2017-09-27T00:14:48-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:94"},{"title":"Reduce la cantidad de pasadas:","text":"val distFile = sc.textFile(\"README.md\")\n\nval cleanFile = distFile.map(l => l.trim)\nval wordsFile = cleanFile.map(l => l.split(\" \"))\n//es lo mismo que\nval wordsFile = distFile.map(l => l.trim.split(\" \"))","user":"anonymous","dateUpdated":"2017-09-27T00:14:55-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndistFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[35] at textFile at <console>:27\n\ncleanFile: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[36] at map at <console>:36\n\nwordsFile: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[37] at map at <console>:37\n\nwordsFile: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[38] at map at <console>:36\n"}]},"apps":[],"jobName":"paragraph_1506481120775_-1439442777","id":"20160714-073345_1463486707","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:14:55-0300","dateFinished":"2017-09-27T00:14:56-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:95"},{"text":"%md\n## ~.- Persistencia\n\nSpark **recomputa** el grafo de dependencias cuando se llama una acción:","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.- Persistencia</h2>\n<p>Spark <strong>recomputa</strong> el grafo de dependencias cuando se llama una acción:</p>\n"}]},"apps":[],"jobName":"paragraph_1506481120778_-1440597024","id":"20160715-115826_18047907","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:96"},{"text":"val input = sc.parallelize(1 to 30)\nval result = input.map(x => x*x)\nprintln(result.count()) // computa el map\nprintln(result.collect().mkString(\",\")) // recomputa el map","user":"anonymous","dateUpdated":"2017-09-27T00:15:15-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ninput: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[39] at parallelize at <console>:27\n\nresult: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[40] at map at <console>:29\n30\n1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256,289,324,361,400,441,484,529,576,625,676,729,784,841,900\n"}]},"apps":[],"jobName":"paragraph_1506481120778_-1440597024","id":"20160714-074122_809060797","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:15:15-0300","dateFinished":"2017-09-27T00:15:16-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:97"},{"text":"val uiPort = sc.getConf.getOption(\"spark.ui.port\").getOrElse(\"4040\")\nprint(s\"\"\"\n%html\n(ver resultado en Spark UI\n<a href=\"http://$uiHost:$uiPort\">http://$uiHost(host):$uiPort(port)</a>)\n<br>\n<br>\nPara evitarlo Spark puede cachear los datos:\n\"\"\")\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"(ver resultado en Spark UI\n<a href=\"http://192.168.1.201:4040\">http://192.168.1.201(host):4040(port)</a>)\n<br>\n<br>\nPara evitarlo Spark puede cachear los datos:\n"}]},"apps":[],"jobName":"paragraph_1506481120779_-1440981772","id":"20160715-120820_147737126","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:98"},{"text":"val input = sc.parallelize(1 to 100)\nval result = input.map(x => x*x).setName(\"cuadrados2\").cache()\nprintln(result.mean()) // computa el map cacheando\nprintln(result.collect().mkString(\",\")) // no recomputa el map\n//result.unpersist()","user":"anonymous","dateUpdated":"2017-09-27T00:15:22-0300","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ninput: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[41] at parallelize at <console>:27\n\nresult: org.apache.spark.rdd.RDD[Int] = cuadrados2 MapPartitionsRDD[42] at map at <console>:29\n3383.5\n1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256,289,324,361,400,441,484,529,576,625,676,729,784,841,900,961,1024,1089,1156,1225,1296,1369,1444,1521,1600,1681,1764,1849,1936,2025,2116,2209,2304,2401,2500,2601,2704,2809,2916,3025,3136,3249,3364,3481,3600,3721,3844,3969,4096,4225,4356,4489,4624,4761,4900,5041,5184,5329,5476,5625,5776,5929,6084,6241,6400,6561,6724,6889,7056,7225,7396,7569,7744,7921,8100,8281,8464,8649,8836,9025,9216,9409,9604,9801,10000\n"}]},"apps":[],"jobName":"paragraph_1506481120779_-1440981772","id":"20160715-121014_1297014641","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:15:22-0300","dateFinished":"2017-09-27T00:15:24-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:99"},{"text":"{\nval uiHost = sc.getConf.getOption(\"spark.driver.host\").getOrElse(\"localhost\")\nval uiPort = sc.getConf.getOption(\"spark.ui.port\").getOrElse(\"4040\")\nprint(s\"\"\"\n%html\nVer resultado en Spark UI\n<a href=\"http://$uiHost:$uiPort/storage\">http://$uiHost(host):$uiPort(port)/storage</a>\n<br>\nObservar tambien green dots en Dag Visualization.\n\"\"\")\n}","dateUpdated":"2017-09-26T23:58:40-0300","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"HTML","data":"Ver resultado en Spark UI\n<a href=\"http://192.168.1.200:4040/storage\">http://192.168.1.200(host):4040(port)/storage</a>\n<br>\nObservar tambien green dots en Dag Visualization.\n"}]},"apps":[],"jobName":"paragraph_1506481120780_-1442905517","id":"20160715-175222_773072638","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:100"},{"title":"características:","text":"%md\n* Cada nodo almacena su partición de datos.\n* Al ejecutar nuevas acciones reusa el resultado (más velocidad).\n* Tolerante a fallas: \n    - si el caché se pierde se recomputa reejecutando las transformaciones\n* Es lazy (se ejecuta con la acción)\n* Métodos:\n    - `cache()` cachea en memoria (JVM heap)\n    - `persist(<nivel de persistencia>)` cachea según parámetro (si es vacio por defecto en memoria)\n    - `unpersist()` borra caché.","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<ul>\n<li>Cada nodo almacena su partición de datos.</li>\n<li>Al ejecutar nuevas acciones reusa el resultado (más velocidad).</li>\n<li>Tolerante a fallas:<ul>\n<li>si el caché se pierde se recomputa reejecutando las transformaciones</li>\n</ul>\n</li>\n<li>Es lazy (se ejecuta con la acción)</li>\n<li>Métodos:<ul>\n<li><code>cache()</code> cachea en memoria (JVM heap)</li>\n<li><code>persist(&lt;nivel de persistencia&gt;)</code> cachea según parámetro (si es vacio por defecto en memoria)</li>\n<li><code>unpersist()</code> borra caché.</li>\n</ul>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481120780_-1442905517","id":"20160715-121506_391366546","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:101"},{"title":"Niveles de persistencia","text":"print(s\"\"\"\n%table\nNivel de almacenamienro \\t Significado\nMEMORY_ONLY \\t Guarda RDD como objetos Java no serializados. Si el RDD no cabe en memoria algunas particiones no serán cacheadas y se recomputarán cuando se necesitan. Es el nivel por defecto.\nMEMORY_AND_DISK \\t Guarda RDD como objetos Java no serializados. Si el RDD no cabe en memoria guarda las particiones que no caben en disco y las lee cuando se necesiten.\nMEMORY_ONLY_SER \\t Guarda RDD como objetos Java serializados (un array de bytes por partición). Es generalmente más eficiente en espacio pero menos eficiente en uso de procesador.\nMEMORY_AND_DISK_SER \\t Similar a MEMORY_ONLY_SER, pero guarda en disco las particiones que no caben en memoria.\nDISK_ONLY \\t Guarda las particiones en disco.\nMEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.\\t Igual que las anteriores pero replica cada partición en dos nodos.\nOFF_HEAP (experimental) \\t Guarda los RDD serializados en Alluxio. Parecido a MEMORY_ONLY_SER pero reduce el tiempor de garbage collection y reduce el tamaño de los ejecutores. Bueno para ambientes con mucha memoria y múltiples aplicaciones concurrentes. En caso de fallas, Spark no necesita reconstruir las particiones (Alluxio tiene su propio mecanismo de tolerancia a fallas).\n\"\"\")\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":366,"optionOpen":false,"keys":[{"name":"Nivel de almacenamienro ","index":0,"aggr":"sum"}],"values":[{"name":" Significado","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Nivel de almacenamienro ","index":0,"aggr":"sum"},"yAxis":{"name":" Significado","index":1,"aggr":"sum"}}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Nivel de almacenamienro \t Significado\nMEMORY_ONLY \t Guarda RDD como objetos Java no serializados. Si el RDD no cabe en memoria algunas particiones no serán cacheadas y se recomputarán cuando se necesitan. Es el nivel por defecto.\nMEMORY_AND_DISK \t Guarda RDD como objetos Java no serializados. Si el RDD no cabe en memoria guarda las particiones que no caben en disco y las lee cuando se necesiten.\nMEMORY_ONLY_SER \t Guarda RDD como objetos Java serializados (un array de bytes por partición). Es generalmente más eficiente en espacio pero menos eficiente en uso de procesador.\nMEMORY_AND_DISK_SER \t Similar a MEMORY_ONLY_SER, pero guarda en disco las particiones que no caben en memoria.\nDISK_ONLY \t Guarda las particiones en disco.\nMEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.\t Igual que las anteriores pero replica cada partición en dos nodos.\nOFF_HEAP (experimental) \t Guarda los RDD serializados en Alluxio. Parecido a MEMORY_ONLY_SER pero reduce el tiempor de garbage collection y reduce el tamaño de los ejecutores. Bueno para ambientes con mucha memoria y múltiples aplicaciones concurrentes. En caso de fallas, Spark no necesita reconstruir las particiones (Alluxio tiene su propio mecanismo de tolerancia a fallas).\n"}]},"apps":[],"jobName":"paragraph_1506481120781_-1443290266","id":"20160718-175802_323407423","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"title":"Notas:","text":"%md\n* [Alluxio](http://www.alluxio.org/docs/master/en/) es una tecnología de almacenamiento compartido en memoria. Anteriormente se llamó Tachyon. Para más información ver [Alluxio Docs: Running Spark on Alluxio](http://www.alluxio.org/docs/master/en/Running-Spark-on-Alluxio.html).\n* En Python no se puede elegir el nivel de almacenamiento (siempre serializa en memoria).\n* Ver también [Spark Docs: Which Storage Level to Choose?](http://spark.apache.org/docs/latest/programming-guide.html#which-storage-level-to-choose).\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<ul>\n<li><a href=\"http://www.alluxio.org/docs/master/en/\">Alluxio</a> es una tecnología de almacenamiento compartido en memoria. Anteriormente se llamó Tachyon. Para más información ver <a href=\"http://www.alluxio.org/docs/master/en/Running-Spark-on-Alluxio.html\">Alluxio Docs: Running Spark on Alluxio</a>.</li>\n<li>En Python no se puede elegir el nivel de almacenamiento (siempre serializa en memoria).</li>\n<li>Ver también <a href=\"http://spark.apache.org/docs/latest/programming-guide.html#which-storage-level-to-choose\">Spark Docs: Which Storage Level to Choose?</a>.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481120782_-1442136019","id":"20160718-183042_280042764","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103"},{"text":"%md\n## ~.- Programas, Jobs, Stages y Tasks \n\nDurante la ejecución de un programa se hace un plan de ejecución según el **grafo de dependencias**:\n\n* Cada **acción** dispara un **job** para obtener el resultado.\n* Cada job es dividido en **stages**, una para cada RDD. \n* Cada stage consiste de multiples **tasks**, una para cada **partición** del RDD.\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.- Programas, Jobs, Stages y Tasks</h2>\n<p>Durante la ejecución de un programa se hace un plan de ejecución según el <strong>grafo de dependencias</strong>:</p>\n<ul>\n<li>Cada <strong>acción</strong> dispara un <strong>job</strong> para obtener el resultado.</li>\n<li>Cada job es dividido en <strong>stages</strong>, una para cada RDD.</li>\n<li>Cada stage consiste de multiples <strong>tasks</strong>, una para cada <strong>partición</strong> del RDD.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481120782_-1442136019","id":"20160729-095410_1619479256","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:104"},{"text":"print(s\"\"\"%html\n&nbsp;\n<img src=\"$baseDir/stage-tasks.png\" alt=\"Drawing\" style=\"width: 100%;\"/>\n\"\"\")","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"&nbsp;\n<img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/04_rdd_notebook/stage-tasks.png\" alt=\"Drawing\" style=\"width: 100%;\"/>\n"}]},"apps":[],"jobName":"paragraph_1506481120783_-1442520768","id":"20160729-104015_819248214","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:105"},{"text":"%md\n\n#### Job:\n* Comienza con el RDD que dispara la acción.\n* Eventualmente incluye otros RDD's según el **grafo de dependencias**.\n\n<br><br><br><br><br><br><br>\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":84,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Job:</h4>\n<ul>\n<li>Comienza con el RDD que dispara la acción.</li>\n<li>Eventualmente incluye otros RDD's según el <strong>grafo de dependencias</strong>.</li>\n</ul>\n<p><br><br><br><br><br><br><br></p>\n"}]},"apps":[],"jobName":"paragraph_1506481120783_-1442520768","id":"20160729-104645_1867345422","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:106"},{"text":"%md\n#### Stage:\n* Subconjunto de operaciones a aplicadar en secuencia.\n* Se delimitan según la necesidad de intercambio de datos entre nodos (ej. `intersection`).\n    - Parten el grafo de dependencias según los puntos de intercambio.\n    - Un stage contiene secuencias de *narrow transformations* unidas en puntos de intercambian datos (*shuffle boundary*).\n* Producen *barreras* donde otras stages deben esperar su resultado.\n* Conforman un grafo nuevo.","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Stage:</h4>\n<ul>\n<li>Subconjunto de operaciones a aplicadar en secuencia.</li>\n<li>Se delimitan según la necesidad de intercambio de datos entre nodos (ej. <code>intersection</code>).<ul>\n<li>Parten el grafo de dependencias según los puntos de intercambio.</li>\n<li>Un stage contiene secuencias de <em>narrow transformations</em> unidas en puntos de intercambian datos (<em>shuffle boundary</em>).</li>\n</ul>\n</li>\n<li>Producen <em>barreras</em> donde otras stages deben esperar su resultado.</li>\n<li>Conforman un grafo nuevo.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481120784_-1432132548","id":"20160729-105310_364353621","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:107"},{"title":"Grafo de Dependencias/Grafo de Stages","text":"print(s\"\"\"%html\n<img src=\"$baseDir/dagscheduler-stages.png\" alt=\"Drawing\" style=\"width: 100%;\"/>\n\"\"\")","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/04_rdd_notebook/dagscheduler-stages.png\" alt=\"Drawing\" style=\"width: 100%;\"/>\n"}]},"apps":[],"jobName":"paragraph_1506481120784_-1432132548","id":"20160729-121539_794985257","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"text":"%md\n####Tasks:\n\n* Partes de un stage como mínimas unidades de ejecución.\n* Son enviadas a los executors en los nodos.\n* Una por partición.\n* En orden según grafo de dependencias.\n* Ejecutan los siguientes pasos:\n    - leen la entrada de datos de almacenamiento externo, otro RDD cacheado o lo producido por tareas de mezcla (task shuffle).\n    - ejecutan las transformaciones (optimizando)\n    - escriben las salida a un mezclador, almacenamiento externo o al driver (por ejecución de acción).","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Tasks:</h4>\n<ul>\n<li>Partes de un stage como mínimas unidades de ejecución.</li>\n<li>Son enviadas a los executors en los nodos.</li>\n<li>Una por partición.</li>\n<li>En orden según grafo de dependencias.</li>\n<li>Ejecutan los siguientes pasos:<ul>\n<li>leen la entrada de datos de almacenamiento externo, otro RDD cacheado o lo producido por tareas de mezcla (task shuffle).</li>\n<li>ejecutan las transformaciones (optimizando)</li>\n<li>escriben las salida a un mezclador, almacenamiento externo o al driver (por ejecución de acción).</li>\n</ul>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506481120785_-1432517297","id":"20160729-122600_874955681","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"text":"val uiHost = sc.getConf.getOption(\"spark.driver.host\").getOrElse(\"localhost\")\nval uiPort = sc.getConf.getOption(\"spark.ui.port\").getOrElse(\"4040\")\nprint(s\"\"\"\n%html\n<br>\nVer la representación de estos conceptos en Spark UI\n<a href=\"http://$uiHost:$uiPort\">http://$uiHost(host):$uiPort(port)</a>\n\"\"\")","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<br>\nVer la representación de estos conceptos en Spark UI\n<a href=\"http://192.168.128.215:4040\">http://192.168.128.215(host):4040(port)</a>\n"}]},"apps":[],"jobName":"paragraph_1506481120785_-1432517297","id":"20160729-124534_976548333","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"text":"%md \n## ~.-Reparticionado\n\nLos RDD se distribuyen en particiones a través de los nodos.\nSegún como cambien los datos (creación de RDD) puede ser util moverlos cambiando las particiones.\n<br>\nEl siguiente ejemplo es un programa que calcula numero primos hasta un `n`:\n* calcula todos los numero que son multiplos de otros dos (compuestos)\n* y los resta a todos los numeros hasta `n`.\n\nCorrer el siguiente ejemplo y ver grafo y *Stages* del *Job* en Spark UI.","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.-Reparticionado</h2>\n<p>Los RDD se distribuyen en particiones a través de los nodos.\n<br  />Según como cambien los datos (creación de RDD) puede ser util moverlos cambiando las particiones.\n<br  /><br>\n<br  />El siguiente ejemplo es un programa que calcula numero primos hasta un <code>n</code>:</p>\n<ul>\n<li>calcula todos los numero que son multiplos de otros dos (compuestos)</li>\n<li>y los resta a todos los numeros hasta <code>n</code>.</li>\n</ul>\n<p>Correr el siguiente ejemplo y ver grafo y <em>Stages</em> del <em>Job</em> en Spark UI.</p>\n"}]},"apps":[],"jobName":"paragraph_1506481120788_-1433671543","id":"20160811-182728_1985106852","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"title":"Ejemplo: Numeros Primos","text":"val n = 2000000\nval compuestos = sc.parallelize(2 to n, 8).map(x => (x, (2 to (n / x)))).flatMap{case (x, ms) => ms.map(_ * x)}\nval primos = sc.parallelize(2 to n, 8).subtract(compuestos)\nprimos.collect()","user":"anonymous","dateUpdated":"2017-09-27T00:15:34-0300","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nn: Int = 2000000\n\ncompuestos: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[47] at flatMap at <console>:29\n\nprimos: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[52] at subtract at <console>:31\nres34: Array[Int] = Array(1048601, 17, 41, 1048633, 1048609, 73, 524353, 89, 524369, 1572929, 97, 113, 1048681, 1048721, 137, 1573009, 193, 1573081, 1048793, 524497, 1573057, 524521, 233, 241, 257, 281, 1048889, 313, 524593, 1573153, 337, 524633, 1048897, 1573193, 353, 1573217, 524681, 401, 409, 433, 449, 457, 1049057, 521, 524801, 1049089, 1049137, 524857, 569, 1049129, 577, 524873, 1049177, 593, 601, 1049201, 617, 524921, 641, 673, 524969, 1573553, 525001, 1049297, 525017, 1049281, 1573577, 761, 769, 809, 857, 525137, 1573753, 881, 525193, 1049497, 525209, 1049473, 929, 937, 525241, 953, 525257, 977, 1049537, 1009, 1049569, 1033, 525313, 1049, 525353, 1573937, 525361, 1573921, 1049681, 1097, 525377, 1573969, 1573961, 1574009, 1129, 525409, 525433, 1153, 525457, 1193, 1201, 1574057, 12..."}]},"apps":[],"jobName":"paragraph_1506481120788_-1433671543","id":"20160811-180542_502600160","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:15:34-0300","dateFinished":"2017-09-27T00:16:00-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"text":"%md\nEn Spark UI se puede ver que:\n* Se crean 3 Stages.\n* En la primera (donde se calcula `compuestos`) una task (un core) hace el 90% del trabajo\n    - las otros esperan.\n\nQue sucede?\n* Al hacer `parallelize` se distrubuyen los datos en particiones (nodos).\n* Al hacer el primer `map`  se alteran creciendo en tamaño los números más grandes: tienen más multiplicadores.\n\nSolución:\n* Volver a particionar después del `map`\n    - Distribuirá mejor los datos y uso de procesamiento\n    - **pero** habrá intercambio de datos entre nodos\n    - **por lo tanto** generará una nueva etapa.\n\nEjecutar el siguiente programa y ver resultado en Spark UI\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>En Spark UI se puede ver que:</p>\n<ul>\n<li>Se crean 3 Stages.</li>\n<li>En la segunda (donde se calcula <code>compuestos</code>) una task (un core) hace el 90% del trabajo<ul>\n<li>las otros esperan.</li>\n</ul>\n</li>\n</ul>\n<p>Que sucede?</p>\n<ul>\n<li>Al hacer <code>parallelize</code> se distrubuyen los datos en particiones (nodos).</li>\n<li>Al hacer el primer <code>map</code>  se alteran creciendo en tamaño los números más grandes: tienen más multiplicadores.</li>\n</ul>\n<p>Solución:</p>\n<ul>\n<li>Volver a particionar después del <code>map</code><ul>\n<li>Distribuirá mejor los datos y uso de procesamiento</li>\n<li><strong>pero</strong> habrá intercambio de datos entre nodos</li>\n<li><strong>por lo tanto</strong> generará una nueva etapa.</li>\n</ul>\n</li>\n</ul>\n<p>Ejecutar el siguiente programa y ver resultado en Spark UI</p>\n"}]},"apps":[],"jobName":"paragraph_1506481120789_-1434056292","id":"20160811-183442_1361422879","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"text":"val n = 2000000\nval compuestos = sc.parallelize(2 to n, 8).map(x => (x, (2 to (n / x)))).repartition(8).flatMap{case (x, ms) => ms.map(_ * x)}\nval primos = sc.parallelize(2 to n, 8).subtract(compuestos)\nprimos.collect()","user":"anonymous","dateUpdated":"2017-09-27T00:16:18-0300","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nn: Int = 2000000\n\ncompuestos: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[59] at flatMap at <console>:29\n\nprimos: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[64] at subtract at <console>:31\nres35: Array[Int] = Array(1048601, 17, 41, 1048633, 1048609, 73, 524353, 89, 524369, 1572929, 97, 113, 1048681, 1048721, 137, 1573009, 193, 1573081, 1048793, 524497, 1573057, 524521, 233, 241, 257, 281, 1048889, 313, 524593, 1573153, 337, 524633, 1048897, 1573193, 353, 1573217, 524681, 401, 409, 433, 449, 457, 1049057, 521, 524801, 1049089, 1049137, 524857, 569, 1049129, 577, 524873, 1049177, 593, 601, 1049201, 617, 524921, 641, 673, 524969, 1573553, 525001, 1049297, 525017, 1049281, 1573577, 761, 769, 809, 857, 525137, 1573753, 881, 525193, 1049497, 525209, 1049473, 929, 937, 525241, 953, 525257, 977, 1049537, 1009, 1049569, 1033, 525313, 1049, 525353, 1573937, 525361, 1573921, 1049681, 1097, 525377, 1573969, 1573961, 1574009, 1129, 525409, 525433, 1153, 525457, 1193, 1201, 1574057, 12..."}]},"apps":[],"jobName":"paragraph_1506481120789_-1434056292","id":"20160811-181651_1061365705","dateCreated":"2017-09-26T23:58:40-0300","dateStarted":"2017-09-27T00:16:18-0300","dateFinished":"2017-09-27T00:16:41-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"title":"FIN","text":"val baseDir=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/04_rdd_notebook\"\nprint(\"\"\"%html\n<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\n\"\"\")\n","dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\n"}]},"apps":[],"jobName":"paragraph_1506481120790_-1432902046","id":"20160719-200319_1498591788","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"dateUpdated":"2017-09-26T23:58:40-0300","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1506481120790_-1432902046","id":"20160801-155852_2095574272","dateCreated":"2017-09-26T23:58:40-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116"}],"name":"Presentación 03 - RDD's","id":"2CUBT5XVB","angularObjects":{"2CQY683MD:shared_process":[],"2CRGPDJTN:shared_process":[],"2CTZD3XCX:shared_process":[],"2CRMCDEQM:shared_process":[],"2CTAV9E28:shared_process":[],"2CRA7KCDA:shared_process":[],"2CQ81X7G5:shared_process":[],"2CRUQ6EVN:shared_process":[],"2CTHYC7X9:shared_process":[],"2CRHWDFYM:shared_process":[],"2CQJ7MXW5:shared_process":[],"2CQAR5VSM:shared_process":[],"2CRCFF6HB:shared_process":[],"2CTK8TR8Q:shared_process":[],"2CQN5Q87B:shared_process":[],"2CTW5YZGW:shared_process":[],"2CSR7EJVT:shared_process":[],"2CRUQ7EXT:shared_process":[],"2CRFFBRXJ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}