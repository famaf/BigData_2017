{"paragraphs":[{"text":"print(\"\"\"%html\n<center>\n    <h1>Programación Distribuida sobre Grandes Volúmenes de Datos</h1>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/comun/logo%20UNC%20FAMAF%202016.svg\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<h4 style=\"text-align:center;\"> Damián Barsotti  </h4>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n\"\"\")\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<center>\n    <h1>Programación Distribuida sobre Grandes Volúmenes de Datos</h1>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/comun/logo%20UNC%20FAMAF%202016.svg\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<h4 style=\"text-align:center;\"> Damián Barsotti  </h4>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n"}]},"apps":[],"jobName":"paragraph_1505942454687_558010768","id":"20160801-144445_2123073314","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:28"},{"text":"%md\n# Key / Value RDD's\n\nUsar datos indexados es la idea principal de los modelos de programación distribuida.\n\n* Permite la distribución de datos y  programas y nodos según claves (key).\n* Se puede ubicar rapidamente datos mediante *hash* de sus claves.\n* Permite definir operaciones comunes a valores con claves iguales.\n* Ej: MapReduce,\n* En Spark: \n    - RDD que contienen pares. \n    - Más operaciones además de MR.\n    - [Api Doc](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions) específica.\n\n\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Key / Value RDD&rsquo;s</h1>\n<p>Usar datos indexados es la idea principal de los modelos de programación distribuida.</p>\n<ul>\n  <li>Permite la distribución de datos y programas y nodos según claves (key).</li>\n  <li>Se puede ubicar rapidamente datos mediante <em>hash</em> de sus claves.</li>\n  <li>Permite definir operaciones comunes a valores con claves iguales.</li>\n  <li>Ej: MapReduce,</li>\n  <li>En Spark:\n    <ul>\n      <li>RDD que contienen pares.</li>\n      <li>Más operaciones además de MR.</li>\n      <li><a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\">Api Doc</a> específica.</li>\n    </ul>\n  </li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1505942454700_539158072","id":"20160801-151104_1224917094","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:29"},{"text":"%md\n## ~.- Creación\n\nLos Key/Value RDD's se crean de la misma forma que un RDD común conteniendo pares: \n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.- Creación</h2>\n<p>Los Key/Value RDD's se crean de la misma forma que un RDD común conteniendo pares:</p>\n"}]},"apps":[],"jobName":"paragraph_1505942454702_539927570","id":"20160801-151220_1226466151","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:30"},{"text":"val lines = sc.textFile(\"README.md\")\n// Pares con primer palabra de la linea y la linea completa:\nval pairs =  lines.filter(! _.trim.isEmpty).map(x => (x.split(\" \")(0), x))\npairs.take(5).foreach(println)\n","user":"anonymous","dateUpdated":"2017-09-20T18:22:07-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","lineNumbers":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:27\n\npairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[3] at map at <console>:30\n(#,# Apache Zeppelin)\n(**Documentation:**,**Documentation:** [User Guide](http://zeppelin.apache.org/docs/latest/index.html)<br/>)\n(**Mailing,**Mailing Lists:** [User and Dev mailing list](http://zeppelin.apache.org/community.html)<br/>)\n(**Continuous,**Continuous Integration:** [![Build Status](https://travis-ci.org/apache/zeppelin.svg?branch=master)](https://travis-ci.org/apache/zeppelin) <br/>)\n(**Contributing:**,**Contributing:** [Contribution Guide](https://zeppelin.apache.org/contribution/contributions.html)<br/>)\n"}]},"apps":[],"jobName":"paragraph_1505942454704_549931042","id":"20160801-152913_893021880","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T18:22:07-0300","dateFinished":"2017-09-20T18:22:41-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:31"},{"text":"\n\nval cuadrados = sc.parallelize(1 to 5).map(x => (x.toString + \":\",  x*x))\ncuadrados.collect","user":"anonymous","dateUpdated":"2017-09-20T18:23:34-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","lineNumbers":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ncuadrados: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[5] at map at <console>:29\n\nres4: Array[(String, Int)] = Array((1:,1), (2:,4), (3:,9), (4:,16), (5:,25))\n"}]},"apps":[],"jobName":"paragraph_1505942454706_550700539","id":"20160801-153626_1555305896","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T18:23:34-0300","dateFinished":"2017-09-20T18:23:36-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:32"},{"text":"%md\n## ~.- Transformaciones\n\nLos Key/Value RDD's tiene las transformaciones ya vistas y otras específicas.\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.- Transformaciones</h2>\n<p>Los Key/Value RDD's tiene las transformaciones ya vistas y otras específicas.</p>\n"}]},"apps":[],"jobName":"paragraph_1505942454708_548392046","id":"20160801-162053_496023389","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:33"},{"text":"val lineasCortas = pairs.filter{case (k,l) => l.length < 20}\nlineasCortas.take(3).foreach(println)\n","user":"anonymous","dateUpdated":"2017-09-20T18:27:00-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlineasCortas: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[8] at filter at <console>:31\n(#,# Apache Zeppelin)\n(Core,Core feature:)\n(##,## Getting Started)\n"}]},"apps":[],"jobName":"paragraph_1505942454709_548007297","id":"20160801-162615_681608006","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T18:27:01-0300","dateFinished":"2017-09-20T18:27:02-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:34"},{"text":"%md\n#### `mapValues()`\n* Toma una función que modifica solo los valores, dejando intacta su clave.\n* Es la particularización de transformación general `map()`  para valores indexados.\n\n\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4><code>mapValues()</code></h4>\n<ul>\n<li>Toma una función que modifica solo los valores, dejando intacta su clave.</li>\n<li>Es la particularización de transformación general <code>map()</code>  para valores indexados.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1505942454710_549161544","id":"20160804-191225_1194988973","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35"},{"text":"val lengthsRDD = pairs.mapValues(_.length)\nlengthsRDD.take(3).foreach(println)\n\nval lengthsRDD2 = lineasCortas.mapValues(_.length)\nlengthsRDD2.take(3).foreach(println)","user":"anonymous","dateUpdated":"2017-09-20T18:29:44-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","lineNumbers":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nlengthsRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[12] at mapValues at <console>:31\n(#,17)\n(**Documentation:**,87)\n(**Mailing,94)\n\nlengthsRDD2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[13] at mapValues at <console>:34\n(#,17)\n(Core,13)\n(##,18)\n"}]},"apps":[],"jobName":"paragraph_1505942454712_546853050","id":"20160804-191904_605266475","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T18:29:44-0300","dateFinished":"2017-09-20T18:29:46-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:36"},{"text":"%md\n### Aggregations\n\nSon operaciones que se realizan sobre los valores que tiene claves iguales.\n\nLas transformaciones (ya vistas) `reduce()`, `fold()` y `agregate()` tiene su contraparte para datos indexados.\n\n#### `reduceByKey(<operador>)`\n\n* Toma un operador conmutativo y asociativo para combinar los valores con igual clave. \n* Claramente el operador debe devolver el tipo del valor.\n* Devuelve un nuevo Key/Value RDD conteniendo el resultado para cada clave.\n* Hace el *reduce* del **MapReduce**.\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Aggregations</h3>\n<p>Son operaciones que se realizan sobre los valores que tiene claves iguales.</p>\n<p>Las transformaciones (ya vistas) <code>reduce()</code>, <code>fold()</code> y <code>agregate()</code> tiene su contraparte para datos indexados.</p>\n<h4><code>reduceByKey(&lt;operador&gt;)</code></h4>\n<ul>\n<li>Toma un operador conmutativo y asociativo para combinar los valores con igual clave.</li>\n<li>Claramente el operador debe devolver el tipo del valor.</li>\n<li>Devuelve un nuevo Key/Value RDD conteniendo el resultado para cada clave.</li>\n<li>Hace el <em>reduce</em> del <strong>MapReduce</strong>.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1505942454713_546468301","id":"20160801-163251_1354317328","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37"},{"title":"Ejemplo reduceByKey","text":"val rdd = sc.parallelize(List((1,2),(3,4),(3,6)))\n\nval res = rdd.reduceByKey((x,y) => x + y)\nval res2 = rdd.reduceByKey(_ + _)\n\nres.collect\nres2.collect\n","user":"anonymous","dateUpdated":"2017-09-22T18:33:44-0300","config":{"lineNumbers":true,"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[25] at parallelize at <console>:27\n\nres: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[26] at reduceByKey at <console>:30\n\nres2: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[27] at reduceByKey at <console>:29\n\nres31: Array[(Int, Int)] = Array((1,2), (3,10))\n\nres32: Array[(Int, Int)] = Array((1,2), (3,10))\n"}]},"apps":[],"jobName":"paragraph_1505942454714_547622548","id":"20160805-124130_957298131","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-22T18:33:44-0300","dateFinished":"2017-09-22T18:33:48-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:38"},{"title":"Word Count","text":"val file = sc.textFile(\"README.md\")\n//val words = file.flatMap(_.split(\" \"))\nval words = file.flatMap(_.split(\" \")).filter(! _.isEmpty) // Le saca los vacios\nval wordCount = words.map(x=>(x,1)).reduceByKey((nx,ny) => nx+ny)\n\nval result = wordCount.takeOrdered(7)(Ordering[Int].reverse.on (_._2))\n","user":"anonymous","dateUpdated":"2017-09-20T18:40:20-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nfile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[43] at textFile at <console>:27\n\nwords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[45] at filter at <console>:30\n\nwordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[47] at reduceByKey at <console>:31\n\nresult: Array[(String, Int)] = Array((from,4), (Zeppelin,3), (to,3), (Apache,3), (and,3), (interactive,2), (Please,2))\n"}]},"apps":[],"jobName":"paragraph_1505942454715_547237799","id":"20160801-164700_515217994","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T18:40:20-0300","dateFinished":"2017-09-20T18:40:21-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:39"},{"title":"Resultado","text":"{\n    val result = wordCount.takeOrdered(z.input(\"# takes:\",\"7\").toString.toInt)(Ordering[Int].reverse.on (_._2))\n    println(\"%table\\nWord\\tCount\")\n    result.foreach{case (w,c) => println(\"\\\"\" + w + \"\\\"\" + \"\\t\" + c)}\n}\n","user":"anonymous","dateUpdated":"2017-09-22T18:53:31-0300","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}}},"graph":{"mode":"multiBarChart","height":294,"optionOpen":false,"keys":[{"name":"Word","index":0,"aggr":"sum"}],"values":[{"name":"Count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Word","index":0,"aggr":"sum"},"yAxis":{"name":"Count","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{"# takes":"8","# takes:":"10"},"forms":{"# takes:":{"name":"# takes:","displayName":"# takes:","type":"input","defaultValue":"7","hidden":false,"$$hashKey":"object:545"}}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Word\tCount\n\"from\"\t4\n\"Apache\"\t3\n\"and\"\t3\n\"to\"\t3\n\"Zeppelin\"\t3\n\"*\"\t2\n\"###\"\t2\n\"notebook\"\t2\n\"Please\"\t2\n\"binary\"\t2\n"}]},"apps":[],"jobName":"paragraph_1505942454716_545314055","id":"20160801-173559_785661345","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T18:41:56-0300","dateFinished":"2017-09-20T18:41:57-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:40"},{"title":"Que hace?","text":"import org.apache.spark.rdd.RDD // Para documentar con los tipos de los RDD\n\nval file = sc.textFile(\"README.md\")\n\nval wordsPos : RDD[(String, Int)] = file.flatMap(_.split(\" \").zipWithIndex) // zipWithIndex numera\n\nval wordsPosCount : RDD[(String, (Int,Int))] = wordsPos.mapValues(pos=>(pos,1)).reduceByKey((x,y)=>(x._1 + y._1, x._2 + y._2))\n\nval representativos = wordsPosCount.filter{case (w,(_,count))=> count > 2}\n\nval res : RDD[(String, Double)]= representativos.mapValues{case (suma,count)=> suma / count.toDouble}\n\nprintln(\"%table\\nWord\\tProm\")\nres.collect.foreach{case (w,v) => println(\"\\\"\" + w + \"\\\"\" + \"\\t\" + v)}\n","user":"anonymous","dateUpdated":"2017-09-20T18:53:46-0300","config":{"lineNumbers":true,"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"multiBarChart","height":294,"optionOpen":false,"keys":[{"name":"Word","index":0,"aggr":"sum"}],"values":[{"name":"Count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Word","index":0,"aggr":"sum"},"yAxis":{"name":"Count","index":1,"aggr":"sum"}}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.rdd.RDD\n\nfile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[87] at textFile at <console>:34\n\nwordsPos: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[88] at flatMap at <console>:36\n\nwordsPosCount: org.apache.spark.rdd.RDD[(String, (Int, Int))] = ShuffledRDD[90] at reduceByKey at <console>:38\n\nrepresentativos: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[91] at filter at <console>:40\n\nres: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[92] at mapValues at <console>:42\n"},{"type":"TABLE","data":"Word\tProm\n\"Apache\"\t4.0\n\"\"\t0.3333333333333333\n\"from\"\t5.25\n\"Zeppelin\"\t5.333333333333333\n\"to\"\t3.6666666666666665\n\"and\"\t13.0\n"}]},"apps":[],"jobName":"paragraph_1505942454717_544929306","id":"20160804-192727_1332932674","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T18:53:47-0300","dateFinished":"2017-09-20T18:53:48-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:41"},{"title":"Más Aggregations","text":"%md\n* `foldByKey(<neutro>,<operador>)`\n    * Toma un neutro con su operador (monoide conmutativo) para combinar los valores con igual clave.\n    * Devuelve un nuevo Key/Value RDD conteniendo el resultado para cada clave.\n    * Es la versión para datos indexados de `fold()`.\n<br>\n* `aggregateByKey(<neutro>)(<seqOp>,<combOp>)`\n    * Es la versión para datos indexados de `aggregate()`.\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li><code>foldByKey(&lt;neutro&gt;,&lt;operador&gt;)</code>\n    <ul>\n      <li>Toma un neutro con su operador (monoide conmutativo) para combinar los valores con igual clave.</li>\n      <li>Devuelve un nuevo Key/Value RDD conteniendo el resultado para cada clave.</li>\n      <li>Es la versión para datos indexados de <code>fold()</code>.<br/><br></li>\n    </ul>\n  </li>\n  <li><code>aggregateByKey(&lt;neutro&gt;)(&lt;seqOp&gt;,&lt;combOp&gt;)</code>\n    <ul>\n      <li>Es la versión para datos indexados de <code>aggregate()</code>.</li>\n    </ul>\n  </li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1505942454718_546083553","id":"20160801-164657_1861724810","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:42"},{"text":"%md\n### Agrupamiento\n\nA veces es necesario agrupar los datos con igual clave sin efectuar operaciones en el grupo.\n\n* `groupByKey()`: agrupa los elementos con igual clave devolviendo una  clave y un *iterador*<sup>1</sup>.\n\n----\n######1: Un iteradores en Scala tiene los métodos `next` y `hasNext` junto a las operaciones de listas (ver en [Documentación Scala](http://www.scala-lang.org/docu/files/collections-api/collections_43.html)). \n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Agrupamiento</h3>\n<p>A veces es necesario agrupar los datos con igual clave sin efectuar operaciones en el grupo.</p>\n<ul>\n<li><code>groupByKey()</code>: agrupa los elementos con igual clave devolviendo una  clave y un <em>iterador</em><sup>1</sup>.</li>\n</ul>\n<hr />\n<h6>1: Un iteradores en Scala tiene los métodos <code>next</code> y <code>hasNext</code> junto a las operaciones de listas (ver en <a href=\"http://www.scala-lang.org/docu/files/collections-api/collections_43.html\">Documentación Scala</a>).</h6>\n"}]},"apps":[],"jobName":"paragraph_1505942454719_545698804","id":"20160805-122750_318119108","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:43"},{"title":"Ejemplo groupByKey","text":"val rdd = sc.parallelize(List((1,2),(3,4),(3,6)))\n\nval res = rdd.groupByKey()\n\nres.collect.foreach(println)\n","user":"anonymous","dateUpdated":"2017-09-20T18:58:23-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[93] at parallelize at <console>:33\n\nres: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[94] at groupByKey at <console>:36\n(1,CompactBuffer(2))\n(3,CompactBuffer(4, 6))\n"}]},"apps":[],"jobName":"paragraph_1505942454720_531463094","id":"20160805-130338_574617461","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T18:58:23-0300","dateFinished":"2017-09-20T18:58:24-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:44"},{"title":"Nota:","text":"%md\nTambién existe la transformación `groupBy(<funcion clave>)`: \n* No necesita de datos indexados. \n* Tiene como parámetro una función que toma un elemento del RDD y devuelve una clave. \n* El resultado es el agrupamiento de los datos según estas claves generadas por la función.\n* Se usa cuando las claves que necesito no están en los datos.\n* Es poco eficiente.","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>También existe la transformación <code>groupBy(&lt;funcion clave&gt;)</code>:</p>\n<ul>\n<li>No necesita de datos indexados.</li>\n<li>Tiene como parámetro una función que toma un elemento del RDD y devuelve una clave.</li>\n<li>El resultado es el agrupamiento de los datos según estas claves generadas por la función.</li>\n<li>Se usa cuando las claves que necesito no están en los datos.</li>\n<li>Es poco eficiente.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1505942454721_531078345","id":"20160805-132200_1469455865","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:45"},{"title":"Otro WordCount","text":"import org.apache.spark.rdd.RDD\n\nwords : RDD[String]\n\nval eqWords : RDD[(String,Iterable[String])] = \n    words.groupBy(_.toLowerCase)\n\neqWords.takeOrdered(15)(Ordering[String].reverse).foreach(println)\n\nval counts : RDD[(String,Int)]= eqWords.mapValues(ws => ws.size)\ncounts.lookup(\"from\")","user":"anonymous","dateUpdated":"2017-09-20T19:06:14-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.rdd.RDD\n\nres146: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[45] at filter at <console>:30\n\neqWords: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[126] at groupBy at <console>:49\n\n\n\n\n\n<console>:51: error: type mismatch;\n found   : scala.math.Ordering[String]\n required: Ordering[(String, Iterable[String])]\n       eqWords.takeOrdered(15)(Ordering[String].reverse).foreach(println)\n                                                ^\n"}]},"apps":[],"jobName":"paragraph_1505942454722_532232592","id":"20160808-141232_522804137","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T19:06:14-0300","dateFinished":"2017-09-20T19:06:15-0300","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:46"},{"title":"Otras Transformaciones...rdd={(1,2),(3,4),(3,6)}","text":"print(s\"\"\"\n%table\nTansformación\\tDescripción\\t Ejemplo \\t Resultado\n%html <b>keys</b>()\\t Devuelve las claves de un RDD. \\t rdd.keys() \\t {1,3,3}\n%html <b>values</b>()\\t Devuelve los valores de un RDD. \\t rdd.values() \\t {2,4,6}\n%html <b>flatMapValues</b>(func)\\t Transforma cada valor del par key/value en una lista\\t rdd.flatMapValues(x => (x to 4)) \\t {(1,2),(1,3),(1,4),(1,5),(3,4),(3,5)}\n        \\t manteniendo la clave original con cada elemento generado. \\t  \\t \n%html <b>sortByKey</b>()\\t Ordena el RDD por sus claves. \\t rdd.values() \\t {(1,2),(3,4),(3,6)}\n\"\"\")\n\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":198,"optionOpen":false,"keys":[{"name":"Tansformación","index":0,"aggr":"sum"}],"values":[{"name":"Descripción","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Tansformación","index":0,"aggr":"sum"},"yAxis":{"name":"Descripción","index":1,"aggr":"sum"}}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"TABLE","data":"Tansformación\tDescripción\t Ejemplo \t Resultado\n%html <b>keys</b>()\t Devuelve las claves de un RDD. \t rdd.keys() \t {1,3,3}\n%html <b>values</b>()\t Devuelve los valores de un RDD. \t rdd.values() \t {2,4,6}\n%html <b>flatMapValues</b>(func)\t Transforma cada valor del par key/value en una lista\t rdd.flatMapValues(x => (x to 4)) \t {(1,2),(1,3),(1,4),(1,5),(3,4),(3,5)}\n        \t manteniendo la clave original con cada elemento generado. \t  \t \n%html <b>sortByKey</b>()\t Ordena el RDD por sus claves. \t rdd.values() \t {(1,2),(3,4),(3,6)}\n"}]},"apps":[],"jobName":"paragraph_1505942454723_531847843","id":"20160808-160411_1280016739","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:47"},{"title":"Transformaciones 2 RDD's...rdd={(1,2),(3,4),(3,6)},other={(3,9)}","text":"print(s\"\"\"\n%table\nTansformación\\tDescripción\\t Ejemplo \\t Resultado\n%html <b>substractByKey</b> \\t Elimina los elementos cuya clave está presente en el otro RDD.\\t rdd.substractByKey(other) \\t {(1,2)} \n%html <b>join</b> \\t Realiza un inner join (solo claves comunes) entre los RDD's.\\t rdd.join(other) \\t {(3,(4,9)), (3,(6,9))} \n%html <b>leftOuterJoin</b> \\t Realiza un join desde las claves que están en el primer RDD.\\t rdd.leftOuterJoin(other) \\t {(1, (2,None)), (3, (4,Some(9))), (3, (6,Some(9)))} \n%html <b>rightOuterJoin</b> \\t Realiza un join desde las claves que están en el otro RDD.\\t rdd.rightOuterJoin(other) \\t {(3, (Some(4),9)), (3, (Some(6),9))}\n%html <b>cogroup</b> \\t Agrupa los valores de ambos RDD que tienen igual clave.\\t rdd.cogroup(other) \\t {(1, ([2],[])), (3, ([4,6],[9]))}\n\"\"\")\n","dateUpdated":"2017-09-20T19:31:48-0300","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false},"helium":{}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":214,"optionOpen":false,"keys":[{"name":"Tansformación","index":0,"aggr":"sum"}],"values":[{"name":"Descripción","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Tansformación","index":0,"aggr":"sum"},"yAxis":{"name":"Descripción","index":1,"aggr":"sum"}}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"TABLE","data":"Tansformación\tDescripción\t Ejemplo \t Resultado\n%html <b>substractByKey</b> \t Elimina los elementos cuya clave está presente en el otro RDD.\t rdd.substractByKey(other) \t {(1,2)} \n%html <b>join</b> \t Realiza un inner join (solo claves comunes) entre los RDD's.\t rdd.join(other) \t {(3,(4,9)), (3,(6,9))} \n%html <b>leftOuterJoin</b> \t Realiza un join desde las claves que están en el primer RDD.\t rdd.leftOuterJoin(other) \t {(1, (2,None)), (3, (4,Some(9))), (3, (6,Some(9)))} \n%html <b>rightOuterJoin</b> \t Realiza un join desde las claves que están en el otro RDD.\t rdd.rightOuterJoin(other) \t {(3, (Some(4),9)), (3, (Some(6),9))}\n%html <b>cogroup</b> \t Agrupa los valores de ambos RDD que tienen igual clave.\t rdd.cogroup(other) \t {(1, ([2],[])), (3, ([4,6],[9]))}\n"}]},"apps":[],"jobName":"paragraph_1505942454724_529924099","id":"20160808-162326_562582381","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:48"},{"title":"Nota:","text":"%md\n`cogroup` puede también tomar hasta 3 parámetros (hace cogroup de hasta 4 RDD).","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><code>cogroup</code> puede también tomar hasta 3 parámetros (hace cogroup de hasta 4 RDD).</p>\n"}]},"apps":[],"jobName":"paragraph_1505942454725_529539350","id":"20160905-171030_1158607578","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:49"},{"text":"%md\n## ~.- Acciones de Key/Value RDD\n\nEstán disponibles las acciones más generales (`count`, `take`, etc.) más otras nuevas:","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.- Acciones de Key/Value RDD</h2>\n<p>Están disponibles las acciones más generales (<code>count</code>, <code>take</code>, etc.) más otras nuevas:</p>\n"}]},"apps":[],"jobName":"paragraph_1505942454727_530308848","id":"20160808-170308_21784369","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:50"},{"title":"Acciones Key/Value RDD's...rdd={(1,2),(3,4),(3,6)}","text":"print(s\"\"\"\n%table\nAcción\\tDescripción\\t Ejemplo \\t Resultado\n%html <b>countByKey</b>() \\t Cuenta los elementos de cada clave. Devuelve un Map\\t rdd.countByKey() \\t {(1 -> 1), (3 -> 2)}\n%html <b>collectAsMap</b>() \\t Devuelve el RDD completa como un Map.\\t rdd.collectAsMap() \\t {(1 -> 1), (3 -> 4), (3 -> 6)}\n%html <b>lookup</b>(key) \\t Devuelve todos los valores asociados a una clave.\\t rdd.lookup(3) \\t [4, 6]\n\"\"\")","dateUpdated":"2017-09-20T18:20:54-0300","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":300,"optionOpen":false}},{"graph":{"mode":"table","height":144,"optionOpen":false,"keys":[{"name":"Acción","index":0,"aggr":"sum"}],"values":[{"name":"Descripción","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Acción","index":0,"aggr":"sum"},"yAxis":{"name":"Descripción","index":1,"aggr":"sum"}}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"TABLE","data":"Acción\tDescripción\t Ejemplo \t Resultado\n%html <b>countByKey</b>() \t Cuenta los elementos de cada clave. Devuelve un Map\t rdd.countByKey() \t {(1 -> 1), (3 -> 2)}\n%html <b>collectAsMap</b>() \t Devuelve el RDD completa como un Map.\t rdd.collectAsMap() \t {(1 -> 1), (3 -> 4), (3 -> 6)}\n%html <b>lookup</b>(key) \t Devuelve todos los valores asociados a una clave.\t rdd.lookup(3) \t [4, 6]\n"}]},"apps":[],"jobName":"paragraph_1505942454728_528385103","id":"20160808-170601_1240232489","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:51"},{"title":"WordCount","text":"val file = sc.textFile(\"README.md\")\nval words = file.flatMap(_.split(\" \"))\nval wordCount = words.map(x=>(x,())).countByKey() // ojo! puede ser grande\n\nprintln(\"\\nLa palabra Apache aparece \" + wordCount(\"Apache\") + \" veces.\")\n","user":"anonymous","dateUpdated":"2017-09-20T19:34:09-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nfile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[128] at textFile at <console>:43\n\nwords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[129] at flatMap at <console>:45\nwordCount: scala.collection.Map[String,Long] = Map(Please -> 2, beautiful -> 1, ### -> 2, \"\" -> 18, support -> 1, Apache -> 3, package -> 1, web-based -> 1, Started -> 1, based -> 1, source -> 1, [install](http://zeppelin.apache.org/docs/snapshot/install/install.html) -> 1, * -> 2, **License:** -> 1, Getting -> 1, Integration:** -> 1, [User -> 2, list](http://zeppelin.apache.org/community.html)<br/> -> 1, [Apache -> 1, binary -> 2, Build -> 1, style -> 1, notebook -> 2, Guide](http://zeppelin.apache.org/docs/latest/index.html)<br/> -> 1, Scala -> 1, You -> 1, editor. -> 1, our -> 1, **Continuous -> 1, data -> 1, package. -> 1, [http://zeppelin.apache.org](http://zeppelin.apache.org) -> 1, Built-in -> 1, site -> 1, [Build -> 1, a -> 1, [Contribution -> 1, SQL, -> 1, build -> 1, **Contrib...\nLa palabra Apache aparece 3 veces.\n"}]},"apps":[],"jobName":"paragraph_1505942454729_528000354","id":"20160808-171309_977986235","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T19:34:09-0300","dateFinished":"2017-09-20T19:34:10-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:52"},{"text":"%md\n## ~.- Partición de Datos\n\n* Se puede aumentar la performance dependiendo de como se distribuyan los datos en los nodos.\n    - minimizar la transferencia de datos por red.\n* Con *key/values* se pueden manejar de forma más precisa.\n\n#### Spark divide los datos en  **particiones**:\n\n* Intenta que tuplas de una misma partición están en un mismo nodo.\n* Cada nodo puede contener más de una partición.\n* El número total de particiones es configurable (por defecto total de cores en cluster).\n\n#### Diferentes tipos de particionados según claves:\n\n* **hash-partition** - claves con el mismo hash están en la misma partición\n* **range-partition** - claves en un mismo rango están en una misma partición, según orden de las claves.\n* Definido por el usuario.\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.- Partición de Datos</h2>\n<ul>\n<li>Se puede aumentar la performance dependiendo de como se distribuyan los datos en los nodos.<ul>\n<li>minimizar la transferencia de datos por red.</li>\n</ul>\n</li>\n<li>Con <em>key/values</em> se pueden manejar de forma más precisa.</li>\n</ul>\n<h4>Spark divide los datos en  <strong>particiones</strong>:</h4>\n<ul>\n<li>Intenta que tuplas de una misma partición están en un mismo nodo.</li>\n<li>Cada nodo puede contener más de una partición.</li>\n<li>El número total de particiones es configurable (por defecto total de cores en cluster).</li>\n</ul>\n<h4>Diferentes tipos de particionados según claves:</h4>\n<ul>\n<li><strong>hash-partition</strong> - claves con el mismo hash están en la misma partición</li>\n<li><strong>range-partition</strong> - claves en un mismo rango están en una misma partición, según orden de las claves.</li>\n<li>Definido por el usuario.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1505942454730_529154601","id":"20160808-173145_644890416","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"title":"Ejemplo","text":"%md\n### Ejemplo (cantidad de tópicos nuevos)\n\n#### Problema:\n\n* dada lista fija de tópicos preferidos para cada usuario\n* y archivos de logs de actividad de usuarios cada 5 minutos, \n* ver la cantidad de veces que los usuarios entran a otros tópicos.  \n\n#### Programa:\n* Spark lee las *(clave,valor)*  directamente de file system distribuido.\n* **No conoce** que claves van a que nodo.\n```scala\nval userData = sc.sequenceFile[UserID, UserInfo](\"hdfs://...\").persist()\n```\n\n<br>\n\n* La siguiente función procesa los logs.\n* Los logs también están en fs distribuido como pares *(clave,valor)*:\n    - la clave es el `UserId`,\n    - el valor es el link que accedió.\n* Es llamada periodicamente cada 5 minutos.\n```scala\ndef processNewLogs(logFileName: String) {\n        val events = sc.sequenceFile[UserID, LinkInfo](logFileName)\n        val joined = userData.join(events)// RDD con pares (UserID, (UserInfo, LinkInfo))\n        val offTopicVisits = joined.filter {\n            case (userId, (userInfo, linkInfo)) => // Expande las tuplas del join\n                !userInfo.topics.contains(linkInfo.topic)\n        }.count()\n        println(\"Número de visitas a tópicos no subscriptos: \" + offTopicVisits)\n}\n```\n\n","user":"anonymous","dateUpdated":"2017-09-20T19:45:17-0300","config":{"lineNumbers":false,"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{"baseDir":"40"},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Ejemplo (cantidad de tópicos nuevos)</h3>\n<h4>Problema:</h4>\n<ul>\n  <li>dada lista fija de tópicos preferidos para cada usuario</li>\n  <li>y archivos de logs de actividad de usuarios cada 5 minutos,</li>\n  <li>ver la cantidad de veces que los usuarios entran a otros tópicos.</li>\n</ul>\n<h4>Programa:</h4>\n<ul>\n  <li>Spark lee las <em>(clave,valor)</em> directamente de file system distribuido.</li>\n  <li>\n  <p><strong>No conoce</strong> que claves van a que nodo.</p>\n  <pre><code class=\"scala\">val userData = sc.sequenceFile[UserID, UserInfo](&quot;hdfs://...&quot;).persist()\n</code></pre></li>\n</ul>\n<br>\n<ul>\n  <li>La siguiente función procesa los logs.</li>\n  <li>Los logs también están en fs distribuido como pares <em>(clave,valor)</em>:\n    <ul>\n      <li>la clave es el <code>UserId</code>,</li>\n      <li>el valor es el link que accedió.</li>\n    </ul>\n  </li>\n  <li>\n  <p>Es llamada periodicamente cada 5 minutos.</p>\n  <pre><code class=\"scala\">def processNewLogs(logFileName: String) {\n    val events = sc.sequenceFile[UserID, LinkInfo](logFileName)\n    val joined = userData.join(events)// RDD con pares (UserID, (UserInfo, LinkInfo))\n    val offTopicVisits = joined.filter {\n        case (userId, (userInfo, linkInfo)) =&gt; // Expande las tuplas del join\n            !userInfo.topics.contains(linkInfo.topic)\n    }.count()\n    println(&quot;Número de visitas a tópicos no subscriptos: &quot; + offTopicVisits)\n}\n</code></pre></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1505942454731_528769852","id":"20160809-112816_960723309","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"text":"%md\n#### El programa es poco eficiente\n\n**Cada vez** que se realiza el `join` (se llama a `processNewLogs`) se transfieren datos de ambos RDD `userData` y `events`:\n* se analiza todo el *grafo de dependencias* (lazy evaluation)\n* se crean *hash* con las claves de ambos RDD\n* se transfieren por red los elementos con igual clave al mismo nodo (ambos RDD)\n* se hace el join entre elementos con igual clave en ese nodo.\n\n\n**Notar** que `userData` es más grande que `events`.\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>El programa es poco eficiente</h4>\n<p><strong>Cada vez</strong> que se realiza el <code>join</code> (se llama a <code>processNewLogs</code>) se transfieren datos de ambos RDD <code>userData</code> y <code>events</code>:</p>\n<ul>\n<li>se analiza todo el <em>grafo de dependencias</em> (lazy evaluation)</li>\n<li>se crean <em>hash</em> con las claves de ambos RDD</li>\n<li>se transfieren por red los elementos con igual clave al mismo nodo (ambos RDD)</li>\n<li>se hace el join entre elementos con igual clave en ese nodo.</li>\n</ul>\n<p><strong>Notar</strong> que <code>userData</code> es más grande que <code>events</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1505942454733_526461359","id":"20160810-093816_1883881050","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"text":"print(s\"\"\"%html\n<img src=\"$baseDir/userdata_events_sin_part.png\" alt=\"Drawing\" style=\"width: 80%;\"/>\n\"\"\")","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/05_key_value_rdd/userdata_events_sin_part.png\" alt=\"Drawing\" style=\"width: 80%;\"/>\n"}]},"apps":[],"jobName":"paragraph_1505942454734_527615605","id":"20160810-095722_1433013254","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"text":"%md\n#### Solución\n\n\nUsar la transformacion `partitionBy`:\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Solución</h4>\n<p>Usar la transformacion <code>partitionBy</code>:</p>\n"}]},"apps":[],"jobName":"paragraph_1505942454735_527230856","id":"20160810-105511_464951690","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"text":"%md\n\n~~~scala\nval sc = new SparkContext(...)\nval userData = sc.sequenceFile[UserID, UserInfo](\"hdfs://...\")\n        .partitionBy(new HashPartitioner(100)) // Crea 100 particiones\n        .persist()\n...\n~~~\n\nEsto hace que Spark \n* transfiera `userData` a nodos según sus claves **una sola vez**\n* aproveche esta información para `join`\n    - solo transfiere datos de `events` (más chico).\n\n**Notar que**\n* Sin `persist` no sirve `partitionBy`\n* `events` no se particiona: se usa solo una vez.\n* 100 suele ser la cantidad de cores en el cluster.","user":"anonymous","dateUpdated":"2017-09-20T19:49:04-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<pre><code class=\"scala\">val sc = new SparkContext(...)\nval userData = sc.sequenceFile[UserID, UserInfo](\"hdfs://...\")\n        .partitionBy(new HashPartitioner(100)) // Crea 100 particiones\n        .persist()\n...\n</code></pre>\n<p>Esto hace que Spark</p>\n<ul>\n<li>transfiera <code>userData</code> a nodos según sus claves <strong>una sola vez</strong></li>\n<li>aproveche esta información para <code>join</code><ul>\n<li>solo transfiere datos de <code>events</code> (más chico).</li>\n</ul>\n</li>\n</ul>\n<p><strong>Notar</strong> que</p>\n<ul>\n<li>Sin <code>persist</code> no sirve <code>partitionBy</code></li>\n<li><code>events</code> no se particiona: se usa solo una vez.</li>\n<li>100 suele ser la cantidad de cores en el cluster.</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1505942454736_537619077","id":"20160810-101129_298855161","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"text":"print(s\"\"\"%html\n<img src=\"$baseDir/userdata_events_con_part.png\" alt=\"Drawing\" style=\"width: 80%;\"/>\n\"\"\")","dateUpdated":"2017-09-20T19:48:35-0300","config":{"colWidth":6,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/05_key_value_rdd/userdata_events_con_part.png\" alt=\"Drawing\" style=\"width: 80%;\"/>\n"}]},"apps":[],"jobName":"paragraph_1505942454738_538388575","id":"20160810-105652_1779547689","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"title":"Particionador","text":"// El particionador es un  atributo de los RDD\nval pares = sc.parallelize(List((1, 1), (2, 2), (3, 3)))\n\npares.partitioner\n\nval paresPart = pares.partitionBy(new org.apache.spark.HashPartitioner(2))\nparesPart.partitioner\n","user":"anonymous","dateUpdated":"2017-09-20T19:50:33-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\npares: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[133] at parallelize at <console>:44\n\nres154: Option[org.apache.spark.Partitioner] = None\n\nparesPart: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[134] at partitionBy at <console>:46\n\nres156: Option[org.apache.spark.Partitioner] = Some(org.apache.spark.HashPartitioner@2)\n"}]},"apps":[],"jobName":"paragraph_1505942454739_538003826","id":"20160810-143649_449457189","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T19:50:33-0300","dateFinished":"2017-09-20T19:50:33-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"text":"%md\n### Operaciones que usan partición\n\nSobre 2 RDD: `cogroup()`, `groupWith()`, `join()`, `leftOutherJoin()`, `rightOuterJoin()`\n\n* Si un RDD es particionado no se transmitirá sus datos.\n* Si ambos tienen igual particionador y si están cacheados en las mismas máquinas (por ej. con `mapValues`) no habra transmision de datos por red. \n\nSobre un RDD: `groupByKey()`, `reduceByKey()`, `combineByKey()` y `lookup()`\n\n* El valor de cada clave es computado *localmente*\n ","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Operaciones que usan partición</h3>\n<p>Sobre 2 RDD: <code>cogroup()</code>, <code>groupWith()</code>, <code>join()</code>, <code>leftOutherJoin()</code>, <code>rightOuterJoin()</code></p>\n<ul>\n<li>Si un RDD es particionado no se transmitirá sus datos.</li>\n<li>Si ambos tienen igual particionador y si están cacheados en las mismas máquinas (por ej. con <code>mapValues</code>) no habra transmision de datos por red.</li>\n</ul>\n<p>Sobre un RDD: <code>groupByKey()</code>, <code>reduceByKey()</code>, <code>combineByKey()</code> y <code>lookup()</code></p>\n<ul>\n<li>El valor de cada clave es computado <em>localmente</em></li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1505942454740_536080081","id":"20160810-112328_1820921199","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"text":"%md\n### Herencia de particionado\n\nSpark *hereda* el particionado si conoce como una operación lo afecta:\n* en el ejemplo anterior el resultado del `join` ya está particionado. \n    - `reduceByKey` sobre este resultado sería mas rápido.\n* Si se hace un `map` se elimina el particionado.\n    - no se conocen las nuevas claves\n    - Spark no chequea si permanecen\n\nSolo las operaciones `cogroup()`, `groupWith()`, `join()`, `leftOuterJoin()`, `rightOuterJoin()`, `groupByKey()`, `reduceByKey()`, `combineByKey()`, `sortByKey()`.\n\nEn operaciones binarias si ambos argumentos tienen particionado, se hereda el primero.\n\nAdemás de las anteriores heredan el particionado `mapValues()` y `flatMapValues()`. \n\n","dateUpdated":"2017-09-20T18:20:54-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Herencia de particionado</h3>\n<p>Spark <em>hereda</em> el particionado si conoce como una operación lo afecta:<br/>* en el ejemplo anterior el resultado del <code>join</code> ya está particionado.<br/> - <code>reduceByKey</code> sobre este resultado sería mas rápido.<br/>* Si se hace un <code>map</code> se elimina el particionado.<br/> - no se conocen las nuevas claves<br/> - Spark no chequea si permanecen</p>\n<p>Solo las operaciones <code>cogroup()</code>, <code>groupWith()</code>, <code>join()</code>, <code>leftOuterJoin()</code>, <code>rightOuterJoin()</code>, <code>groupByKey()</code>, <code>reduceByKey()</code>, <code>combineByKey()</code>, <code>sortByKey()</code>.</p>\n<p>En operaciones binarias si ambos argumentos tienen particionado, se hereda el primero.</p>\n<p>Además de las anteriores heredan el particionado <code>mapValues()</code> y <code>flatMapValues()</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1505942454742_536849579","id":"20160810-143435_1857020626","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"text":"%md\n### Ejemplo PageRank\n\nDado dataset con `(pageId, linkList)` y `rank` para cada página:\n<br>\n1. Inicializa cada `rank` a `1.0`\n2. En cada iteración página `p` envía una contribución de `rank(p)/numVecinos(p)` a sus vecinos (páginas a las que `p` apunta)\n3. Poner rank de cada página en `0.15 + 0.85 * contribucionRecibida`\n\nLos últimos dos pasos se ejcutan varias veces (10 lo usual).","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Ejemplo PageRank</h3>\n<p>Dado dataset con <code>(pageId, linkList)</code> y <code>rank</code> para cada página:\n<br  /><br></p>\n<ol>\n<li>Inicializa cada <code>rank</code> a <code>1.0</code></li>\n<li>En cada iteración página <code>p</code> envía una contribución de <code>rank(p)/numVecinos(p)</code> a sus vecinos (páginas a las que <code>p</code> apunta)</li>\n<li>Poner rank de cada página en <code>0.15 + 0.85 * contribucionRecibida</code></li>\n</ol>\n<p>Los últimos dos pasos se ejcutan varias veces (10 lo usual).</p>\n"}]},"apps":[],"jobName":"paragraph_1505942454743_536464830","id":"20160811-163436_1630727393","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"text":"import org.apache.spark.HashPartitioner\n\nval links = sc.textFile(\"./doc/pageRank.txt\")\n    .map(_.split(\" \")).map(ws => (ws.head,ws.tail))\n    .partitionBy(new HashPartitioner(100))\n    .persist()\n\n// Inicialiso cada rank en 1.0.\n// Como usamos mapValues ranks tiene la misma particion que links\nvar ranks = links.mapValues(v => 1.0)\n\n// Corremos 10 iteraciones de PageRank\nfor (i <- 0 until 10) {\n    val contributions = links.join(ranks).flatMap {\n        case (pageId, (links, rank)) =>\n        links.map(dest => (dest, rank / links.size))\n    }\n    ranks = contributions.reduceByKey((x, y) => x + y).mapValues(v => 0.15 + 0.85*v)\n}\n\n// Borramos archivo si existe\ndef rmDir(dir: String) : Unit = { import sys.process._ ;(\"rm -rf \" + dir).!!}\nrmDir(\"ranks.json\")\n\n// Guardamos el resultado\nranks.toDF().write.json(\"ranks.json\")\n\nlinks.unpersist()\n","user":"anonymous","dateUpdated":"2017-09-20T19:58:06-0300","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"graph":{"mode":"table","height":117,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.HashPartitioner\n\nlinks: org.apache.spark.rdd.RDD[(String, Array[String])] = ShuffledRDD[207] at partitionBy at <console>:48\n\nranks: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[208] at mapValues at <console>:50\n\nrmDir: (dir: String)Unit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)\n  at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:473)\n  ... 46 elided\nCaused by: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/mario/Documentos/FaMAF/Optativas/BigData_2017/zeppelin-0.7.2-bin-all/doc/pageRank.txt\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n  at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:91)\n  at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:91)\n  at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:239)\n  at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:237)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.dependencies(RDD.scala:237)\n  at org.apache.spark.scheduler.DAGScheduler.getShuffleDependencies(DAGScheduler.scala:424)\n  at org.apache.spark.scheduler.DAGScheduler.getMissingAncestorShuffleDependencies(DAGScheduler.scala:391)\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getOrCreateShuffleMapStage(DAGScheduler.scala:298)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$getOrCreateParentStages$1.apply(DAGScheduler.scala:374)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$getOrCreateParentStages$1.apply(DAGScheduler.scala:373)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:46)\n  at scala.collection.SetLike$class.map(SetLike.scala:92)\n  at scala.collection.mutable.AbstractSet.map(Set.scala:46)\n  at org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:373)\n  at org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:360)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:838)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1613)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127)\n  ... 66 more\n"}]},"apps":[],"jobName":"paragraph_1505942454744_534541086","id":"20160810-153609_1054835708","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T19:58:06-0300","dateFinished":"2017-09-20T19:58:09-0300","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"{\nval result = sqlContext.read.json(\"ranks.json\").sort($\"_2\".desc).take(z.input(\"# takes ordered:\",\"7\").toString.toInt)\n//deprecated: val result = sqlContext.load(\"ranks.json\",\"json\").sort($\"_2\".desc).take(z.input(\"# takes:\",\"7\").toString.toInt)\nprintln(\"%table\\nLink\\tRank\")\nresult.foreach(l => println(\"\\\"\" + l(0) + \"\\\"\" + \"\\t\" + l(1)))\n}","user":"anonymous","dateUpdated":"2017-09-20T19:58:13-0300","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"graph":{"mode":"pieChart","height":300,"optionOpen":false,"keys":[{"name":"Link","index":0,"aggr":"sum"}],"values":[{"name":"Rank","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Link","index":0,"aggr":"sum"},"yAxis":{"name":"Rank","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{"# takes:":"2","# takes ordered:":"10"},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\n\n\n\n\n\n\n\n\n\norg.apache.spark.sql.AnalysisException: Unable to infer schema for JSON. It must be specified manually.;\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:189)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:189)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$getOrInferFileFormatSchema(DataSource.scala:188)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:387)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:298)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:251)\n  ... 46 elided\n"}]},"apps":[],"jobName":"paragraph_1505942454746_535310583","id":"20160810-171522_2126534025","dateCreated":"2017-09-20T18:20:54-0300","dateStarted":"2017-09-20T19:58:14-0300","dateFinished":"2017-09-20T19:58:14-0300","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"title":"Notar","text":"%md\n* Se hace `join` entre `links` y `ranks`. Como `links` es constante se lo particiona y cachea.\n* `links` es más grande y gracias al particionado no se transfiere por red.\n* Al crear `ranks` hereda el particionado de `links` (por el `mapValues`).\n     - La primera iteración es muy rápida: nada se transmite.\n* `ranks` permanece particionado por hash (todas operaciones *byKey*).","dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<ul>\n<li>Se hace <code>join</code> entre <code>links</code> y <code>ranks</code>. Como <code>links</code> es constante se lo particiona y cachea.</li>\n<li><code>links</code> es más grande y gracias al particionado no se transfiere por red.</li>\n<li>Al crear <code>ranks</code> hereda el particionado de <code>links</code> (por el <code>mapValues</code>).<ul>\n<li>La primera iteración es muy rápida: nada se transmite.</li>\n</ul>\n</li>\n<li><code>ranks</code> permanece particionado por hash (todas operaciones <em>byKey</em>).</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1505942454747_534925834","id":"20160811-171754_2028639705","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"title":"FIN","text":"val baseDir=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/05_key_value_rdd\"\nz.put(\"baseDir\", baseDir)\nprint(\"\"\"%html\n<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\n\"\"\")","dateUpdated":"2017-09-20T18:20:54-0300","config":{"tableHide":true,"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nbaseDir: String = https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/05_key_value_rdd\n"},{"type":"HTML","data":"<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\n"}]},"apps":[],"jobName":"paragraph_1505942454748_533002090","id":"20160801-144602_2031729438","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"dateUpdated":"2017-09-20T18:20:54-0300","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505942454750_533771588","id":"20160810-111427_613288715","dateCreated":"2017-09-20T18:20:54-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:68"}],"name":"Presentación 04 - Key Value RDD's","id":"2CU29FZDH","angularObjects":{"2CQY683MD:shared_process":[],"2CRGPDJTN:shared_process":[],"2CTZD3XCX:shared_process":[],"2CRMCDEQM:shared_process":[],"2CTAV9E28:shared_process":[],"2CRA7KCDA:shared_process":[],"2CQ81X7G5:shared_process":[],"2CRUQ6EVN:shared_process":[],"2CTHYC7X9:shared_process":[],"2CRHWDFYM:shared_process":[],"2CQJ7MXW5:shared_process":[],"2CQAR5VSM:shared_process":[],"2CRCFF6HB:shared_process":[],"2CTK8TR8Q:shared_process":[],"2CQN5Q87B:shared_process":[],"2CTW5YZGW:shared_process":[],"2CSR7EJVT:shared_process":[],"2CRUQ7EXT:shared_process":[],"2CRFFBRXJ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}