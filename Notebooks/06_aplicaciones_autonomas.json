{"paragraphs":[{"text":"print(\"\"\"%html\n<center>\n    <h1>Programación Distribuida sobre Grandes Volúmenes de Datos</h1>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/comun/logo%20UNC%20FAMAF%202016.svg\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<h4 style=\"text-align:center;\"> Damián Barsotti - Ezequiel Orbe  </h4>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n\"\"\")\n","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<center>\n    <h1>Programación Distribuida sobre Grandes Volúmenes de Datos</h1>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://cs.famaf.unc.edu.ar/~damian/bigdata/curso/posgrado_optativa/lectivo/presentaciones/comun/logo%20UNC%20FAMAF%202016.svg\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<h4 style=\"text-align:center;\"> Damián Barsotti - Ezequiel Orbe  </h4>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n"}]},"apps":[],"jobName":"paragraph_1506720531870_-1233319219","id":"20160830-160425_56195218","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:26"},{"text":"%md\n# Aplicaciones autónomas\n\nComo ya vimos Spark puede correr:\n* de forma interactiva con shell (data exploration, prototyping)\n* como una **aplicación autónoma** (standalone) Scala, Python o Java (job recurrente).\n\nNos concentraremos en como construir una aplicación en Scala.\n\n","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Aplicaciones autónomas</h1>\n<p>Como ya vimos Spark puede correr:</p>\n<ul>\n<li>de forma interactiva con shell (data exploration, prototyping)</li>\n<li>como una <strong>aplicación autónoma</strong> (standalone) Scala, Python o Java (job recurrente).</li>\n</ul>\n<p>Nos concentraremos en Scala.</p>\n"}]},"apps":[],"jobName":"paragraph_1506720531885_-1252941413","id":"20160830-160523_1822218154","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:27"},{"text":"%md\n## ~.- Entorno de Desarrollo y Setup\n\nPara desarrollar aplicaciones de Spark en Scala necesitaremos los siguiente:\n\n* Scala\n* SBT\n* Scala IDE (opcional)\n* Spark ","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.- Entorno de Desarrollo y Setup</h2>\n<p>Para desarrollar aplicaciones de Spark en Scala necesitaremos los siguiente:</p>\n<ul>\n<li>Scala</li>\n<li>SBT</li>\n<li>Scala IDE (opcional)</li>\n<li>Spark</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506720531887_-1252171915","id":"20160920-202245_1044937570","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:28"},{"text":"%md\n\n### Scala IDE\n\n* Es un full IDE para desarrollar en scala.\n\n* Se puede descargar desde: [http://scala-ide.org/](http://scala-ide.org/)\n\n* **Pros**:\n    * Intellisense \n    * Integracion con para correr unit tests.\n    * etc.\n\n* **Cons**: \n    * Memory-Intensive (es \"pesado\").","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Scala IDE</h3>\n<ul>\n<li><p>Es un full IDE para desarrollar en scala.</p>\n</li>\n<li><p>Se puede descargar desde: <a href=\"http://scala-ide.org/\">http://scala-ide.org/</a></p>\n</li>\n<li><p><strong>Pros</strong>:</p>\n<ul>\n<li>Intellisense</li>\n<li>Integracion con para correr unit tests.</li>\n<li>etc.</li>\n</ul>\n</li>\n<li><p><strong>Cons</strong>:</p>\n<ul>\n<li>Memory-Intensive (es &ldquo;pesado&rdquo;).</li>\n</ul>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506720531889_-1242168444","id":"20160920-204557_790425937","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:29"},{"text":"%md \n\n### Estructura del Proyecto.\n\n* La estructura recomendada sigue los lineamientos sugeridos por [Maven](http://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html)\n\n```\n<project>\n    - src\n        - main\n            - resources \n            - scala\n        - test\n            - resources \n            - scala\n    - lib\n    - project\n    - target\n    - doc\n    - <project>.sbt\n```","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Estructura del Proyecto.</h3>\n<ul>\n<li>La estructura recomendada sigue los lineamientos sugeridos por <a href=\"http://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html\">Maven</a></li>\n</ul>\n<pre><code>&lt;project&gt;\n    - src\n        - main\n            - resources \n            - scala\n        - test\n            - resources \n            - scala\n    - lib\n    - project\n    - target\n    - doc\n    - &lt;project&gt;.sbt\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1506720531893_-1243707439","id":"20160920-205118_1927997315","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:30"},{"text":"%md\n### Sbt\n\nEs una herramienta de compilación y gestión de dependencias para aplicaciones Scala.\n\n#### Instalación\n\n* Debian/Ubuntu\n```shell-session\n$ echo \"deb https://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list\n$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n$ sudo apt-get update\n$ sudo apt-get install sbt\n```\n\n* Mac\n\n```shell\nbrew install sbt\n```\n\n* Windows MSI en [https://dl.bintray.com/sbt/native-packages/sbt/0.13.x/](https://dl.bintray.com/sbt/native-packages/sbt/0.13.x/)\n\n* `tar.gz` y `zip` en [http://www.scala-sbt.org/download.html](http://www.scala-sbt.org/download.html)\n\n#### Plugins Utiles\n\n* **sbteclipse:** Para generar proyectos de Eclipse.\n* **sbt-assembly:** Para crear \"fat\" JARS.\n\n#### Comandos Utiles\n\n* **sbt update:** Actualiza las dependencias.\n\n* **sbt eclipse:** Crea un proyecto de Eclipse y actualiza las referencias del proyecto.\n\n* **sbt package:** Empaqueta la aplicación creando un JAR (no fat!).\n\n* **sbt assembly:** Empaqueta la aplicación creando un fat JAR. \n","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Sbt</h3>\n<p>Es una herramienta de compilación y gestión de dependencias para aplicaciones Scala.</p>\n<h4>Instalación</h4>\n<ul>\n<li><p>Debian/Ubuntu</p>\n<pre><code class=\"shell-session\">$ echo \"deb https://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list\n$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\n$ sudo apt-get update\n$ sudo apt-get install sbt\n</code></pre>\n</li>\n<li><p>Mac</p>\n</li>\n</ul>\n<pre><code class=\"shell\">brew install sbt\n</code></pre>\n<ul>\n<li><p>Windows MSI en <a href=\"https://dl.bintray.com/sbt/native-packages/sbt/0.13.x/\">https://dl.bintray.com/sbt/native-packages/sbt/0.13.x/</a></p>\n</li>\n<li><p><code>tar.gz</code> y <code>zip</code> en <a href=\"http://www.scala-sbt.org/download.html\">http://www.scala-sbt.org/download.html</a></p>\n</li>\n</ul>\n<h4>Plugins Utiles</h4>\n<ul>\n<li><strong>sbteclipse:</strong> Para generar proyectos de Eclipse.</li>\n<li><strong>sbt-assembly:</strong> Para crear &ldquo;fat&rdquo; JARS.</li>\n</ul>\n<h4>Comandos Utiles</h4>\n<ul>\n<li><p><strong>sbt update:</strong> Actualiza las dependencias.</p>\n</li>\n<li><p><strong>sbt eclipse:</strong> Crea un proyecto de Eclipse y actualiza las referencias del proyecto.</p>\n</li>\n<li><p><strong>sbt package:</strong> Empaqueta la aplicación creando un JAR (no fat!).</p>\n</li>\n<li><p><strong>sbt assembly:</strong> Empaqueta la aplicación creando un fat JAR.</p>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506720531896_-1244861686","id":"20160830-173003_2005788781","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:31"},{"text":"%md\n\n### Contenido básico del archivo .sbt\n\n```scala\nname := \"<project>\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.10.5\"\n\nlibraryDependencies +=  \"org.apache.spark\" %% \"spark-core\" % \"1.6.1\" % \"provided\"\n```","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Contenido básico del archivo .sbt</h3>\n<pre><code class=\"scala\">name := \"&lt;project&gt;\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.10.5\"\n\nlibraryDependencies +=  \"org.apache.spark\" %% \"spark-core\" % \"1.6.1\" % \"provided\"\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1506720531898_-1244092188","id":"20160921-000303_1324292834","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:32"},{"text":"%md\n#### Agregando dependencias en el archivo .sbt\n\n* Las dependencias se agregan a través de `libraryDependencies`.\n\n* En `libraryDependencies` a una librería se le puede indicar una versión de scala específica:\n<br>\n```\nlibraryDependencies +=  \"org.apache.spark\" % \"spark-core_2.9\" % \"1.6.1\" % \"provided\n```\n    - notar que se usa simple `%`\n    - si se usa `%%` sbt agrega automaticamente la version declarada en `scalaVersion := xx.xx.xx`\n<br>\n* A `libraryDependencies` se le puede asignar una secuencia de dependencias:\n<br>\n```scala\nlibraryDependencies ++= Seq(\n    // Spark dependencies\n    \"org.apache.spark\" %% \"spark-core\" % \"1.6.1\" % \"provided\",\n    \"joda-time\" % \"joda-time\" % \"2.0\"\n)\n```\n\n* `provided` indica que no empaquete la librería: usa la del ambiente de ejecución Spark.\n* Más información de sbt en\n    - [http://michele.sciabarra.com/2015/12/11/scala/SBT-from-scratch/](http://michele.sciabarra.com/2015/12/11/scala/SBT-from-scratch/)\n    - [http://www.scala-sbt.org/0.12.4/docs/Detailed-Topics/Library-Management.html](http://www.scala-sbt.org/0.12.4/docs/Detailed-Topics/Library-Management.html)\n    - [https://www.playframework.com/documentation/2.1.1/SBTDependencies](https://www.playframework.com/documentation/2.1.1/SBTDependencies)\n","dateUpdated":"2017-09-29T18:28:51-0300","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Agregando dependencias en el archivo .sbt</h4>\n<ul>\n<li><p>Las dependencias se agregan a través de <code>libraryDependencies</code>.</p>\n</li>\n<li><p>En <code>libraryDependencies</code> a una librería se le puede indicar una versión de scala específica:\n<br  /><br></p>\n<pre><code>libraryDependencies +=  \"org.apache.spark\" % \"spark-core_2.9\" % \"1.6.1\" % \"provided\n</code></pre>\n<ul>\n<li>notar que se usa simple <code>%</code></li>\n<li>si se usa <code>%%</code> sbt agrega automaticamente la version declarada en <code>scalaVersion := xx.xx.xx</code>\n<br  /><br></li>\n</ul>\n</li>\n<li><p>A <code>libraryDependencies</code> se le puede asignar una secuencia de dependencias:\n<br  /><br></p>\n<pre><code class=\"scala\">libraryDependencies ++= Seq(\n// Spark dependencies\n\"org.apache.spark\" %% \"spark-core\" % \"1.6.1\" % \"provided\",\n\"joda-time\" % \"joda-time\" % \"2.0\"\n)\n</code></pre>\n</li>\n<li><p><code>provided</code> indica que no empaquete la librería: usa la del ambiente de ejecución Spark.</p>\n</li>\n<li><p>Más información de sbt en</p>\n<ul>\n<li><a href=\"http://michele.sciabarra.com/2015/12/11/scala/SBT-from-scratch/\">http://michele.sciabarra.com/2015/12/11/scala/SBT-from-scratch/</a></li>\n<li><a href=\"http://www.scala-sbt.org/0.12.4/docs/Detailed-Topics/Library-Management.html\">http://www.scala-sbt.org/0.12.4/docs/Detailed-Topics/Library-Management.html</a></li>\n<li><a href=\"https://www.playframework.com/documentation/2.1.1/SBTDependencies\">https://www.playframework.com/documentation/2.1.1/SBTDependencies</a></li>\n</ul>\n</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1506720531899_-1244476937","id":"20160901-180742_1375631040","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:33"},{"text":"%md\n\n### Script de setup para el haragan :P\n\n```bash\n#!/bin/bash\n\nPROJECT=$1\n\nmkdir -p $PROJECT/src/{main,test}/{java,resources,scala}\nmkdir $PROJECT/{lib,project,target,doc}\n\nVERSION=$(scala -version 2>&1 | sed 's/^.*version \\([0-9.]*\\).*/\\1/')\n\necho 'name := \"'$PROJECT'\"' > $PROJECT/$PROJECT.sbt\necho 'version := \"1.0\"' >> $PROJECT/$PROJECT.sbt\necho 'scalaVersion := \"'$VERSION'\"' >> $PROJECT/$PROJECT.sbt\necho 'libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.6.1\" % \"provided\"' >> $PROJECT/$PROJECT.sbt\n\necho 'bin/\ntarget/' >> $PROJECT/.gitignore\n\n#change to project dir\ncd $PROJECT\n#create eclipse files.\nsbt eclipse\n```\n","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Script de setup para el haragan :P</h3>\n<pre><code class=\"bash\">#!/bin/bash\n\nPROJECT=$1\n\nmkdir -p $PROJECT/src/{main,test}/{java,resources,scala}\nmkdir $PROJECT/{lib,project,target,doc}\n\nVERSION=$(scala -version 2&gt;&amp;1 | sed 's/^.*version \\([0-9.]*\\).*/\\1/')\n\necho 'name := \"'$PROJECT'\"' &gt; $PROJECT/$PROJECT.sbt\necho 'version := \"1.0\"' &gt;&gt; $PROJECT/$PROJECT.sbt\necho 'scalaVersion := \"'$VERSION'\"' &gt;&gt; $PROJECT/$PROJECT.sbt\necho 'libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.6.1\" % \"provided\"' &gt;&gt; $PROJECT/$PROJECT.sbt\n\necho 'bin/\ntarget/' &gt;&gt; $PROJECT/.gitignore\n\n#change to project dir\ncd $PROJECT\n#create eclipse files.\nsbt eclipse\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1506720531904_-1260251642","id":"20160921-000917_1193350083","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:34"},{"text":"%md\n## ~.- Aplicación (Job) de Spark\n\n* Toda aplicación debe tener un \"punto de entrada\" definido mediante el método `main`:\n\n* El `main()` es ejecutado por el **driver**.\n<br>\n\n```scala\n/* CuentaAB.scala */\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\nobject CuentaAB {\n  def main(args: Array[String]) {\n    val logFile = args(0) // Un archivo en el file system\n    val conf = new SparkConf().setAppName(\"Cuenta a y b\")\n    val sc = new SparkContext(conf)\n    val logData = sc.textFile(logFile, 2).cache()\n    val numAs = logData.filter(line => line.contains(\"a\")).count()\n    val numBs = logData.filter(line => line.contains(\"b\")).count()\n    println(\"Lineas con a: %s, Lineas con b: %s\".format(numAs, numBs))\n  }\n}\n```\n\n**Nota:** Se usa `main()` y no una subclase de `scala.App`.","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>~.- Aplicación (Job) de Spark</h2>\n<ul>\n<li><p>Toda aplicación debe tener un &ldquo;punto de entrada&rdquo; definido mediante el método <code>main</code>:</p>\n</li>\n<li><p>El <code>main()</code> es ejecutado por el <strong>driver</strong>.\n<br  /><br></p>\n</li>\n</ul>\n<pre><code class=\"scala\">/* CuentaAB.scala */\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\nobject CuentaAB {\n  def main(args: Array[String]) {\n    val logFile = args(0) // Un archivo en el file system\n    val conf = new SparkConf().setAppName(\"Cuenta a y b\")\n    val sc = new SparkContext(conf)\n    val logData = sc.textFile(logFile, 2).cache()\n    val numAs = logData.filter(line =&gt; line.contains(\"a\")).count()\n    val numBs = logData.filter(line =&gt; line.contains(\"b\")).count()\n    println(\"Lineas con a: %s, Lineas con b: %s\".format(numAs, numBs))\n  }\n}\n</code></pre>\n<p><strong>Nota:</strong> Se usa <code>main()</code> y no una subclase de <code>scala.App</code>.</p>\n"}]},"apps":[],"jobName":"paragraph_1506720531912_-1263329633","id":"20160830-161420_964310911","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:35"},{"text":"%md\n### SparkContext\n\n* El uso de las API's (ej, RDD) dentro de una aplicación autónoma es el mismo que se hace en una sesión interactiva.\n* Pero, hay que crear manualmente el `SparkContext`.\n* En modo interactivo vimos que ya está definido el objeto `sc` de tipo `SparkContext`.\n* Para una aplicación autónoma hay que crearlo:\n\n```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\n...\nval conf = new SparkConf().setAppName(\"Mi Applicacion\")\nval sc = new SparkContext(conf)\n...\n```\n\nEn el ejemplo \n* se importan las clases necesarias\n* se crea el `SparkConf` con la configuración de la aplicación\n    - se indica el nombre que identificará la aplicación en **SparkUI**\n* se crea el `SparkContext` pasándole como parámetro la configuración\n\nTambién se puede indicar el URL del cluster:\n```scala\nval conf = new SparkConf().setAppName(\"Mi Applicacion\").setMaster(\"local[*]\")\n```\n\naunque puede ser conveniente no harcodearlo en la aplicación.","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>SparkContext</h3>\n<ul>\n<li>El uso de las API's (ej, RDD) dentro de una aplicación autónoma es el mismo que se hace en una sesión interactiva.</li>\n<li>Pero, hay que crear manualmente el <code>SparkContext</code>.</li>\n<li>En modo interactivo vimos que ya está definido el objeto <code>sc</code> de tipo <code>SparkContext</code>.</li>\n<li>Para una aplicación autónoma hay que crearlo:</li>\n</ul>\n<pre><code class=\"scala\">import org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\n...\nval conf = new SparkConf().setAppName(\"Mi Applicacion\")\nval sc = new SparkContext(conf)\n...\n</code></pre>\n<p>En el ejemplo</p>\n<ul>\n<li>se importan las clases necesarias</li>\n<li>se crea el <code>SparkConf</code> con la configuración de la aplicación<ul>\n<li>se indica el nombre que identificará la aplicación en <strong>SparkUI</strong></li>\n</ul>\n</li>\n<li>se crea el <code>SparkContext</code> pasándole como parámetro la configuración</li>\n</ul>\n<p>También se puede indicar el URL del cluster:</p>\n<pre><code class=\"scala\">val conf = new SparkConf().setAppName(\"Mi Applicacion\").setMaster(\"local[*]\")\n</code></pre>\n<p>aunque puede ser conveniente no harcodearlo en la aplicación.</p>\n"}]},"apps":[],"jobName":"paragraph_1506720531913_-1263714382","id":"20160830-160603_1923521712","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:36"},{"text":"%md\n#### Empaquetado\n\n* Al enviarse una aplicación a Spark también se deben proveer todas las librerías del *grafo de dependencias transitivas*:\n    - no solo las librerías que usamos si no las que ellas usan.\n\n* Lo mas conveniente es crear un fat JAR.\n\nEn la raiz del proyecto:\n\n```console\n$ sbt assembly\n[info] Loading global plugins from /.sbt/0.13/plugins\n[info] Set current project to cuentaAB (in build file:tmp/cuentaAB/)\n[info] Updating {file:tmp/cuentaAB/}cuentaab...\n[info] Resolving org.scala-lang#scalap;2.10.5 ...\n[info] Done updating.\n[info] Including from cache: scala-library-2.10.5.jar\n[info] Checking every *.class/*.jar file's SHA-1.\n[info] Merging files...\n[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'\n[warn] Strategy 'discard' was applied to a file\n[info] SHA-1: fccf7195b12e31b9c7f956504a8341b65057e2e2\n[info] Packaging tmp/cuentaAB/target/scala-2.10/cuentaAB-assembly-1.0.jar ...\n[info] Done packaging.\n[success] Total time: 7 s, completed Sep 21, 2016 8:47:39 AM\n\n```\n\n* Si se esta probando la aplicación en un entorno local, se puede evitar la creación de un fat JAR. \n\n```console\n$ sbt package\n[info] Loading global plugins from .sbt/0.13/plugins\n[info] Set current project to cuentaAB (in build file:/tmp/cuentaAB/)\n[info] Packaging tmp/cuentaAB/target/scala-2.10/cuentaab_2.10-1.0.jar ...\n[info] Done packaging.\n[success] Total time: 0 s, completed Sep 21, 2016 8:50:35 AM\n```\n\n","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Empaquetado</h4>\n<ul>\n<li><p>Al enviarse una aplicación a Spark también se deben proveer todas las librerías del <em>grafo de dependencias transitivas</em>:</p>\n<ul>\n<li>no solo las librerías que usamos si no las que ellas usan.</li>\n</ul>\n</li>\n<li><p>Lo mas conveniente es crear un fat JAR.</p>\n</li>\n</ul>\n<p>En la raiz del proyecto:</p>\n<pre><code class=\"console\">$ sbt assembly\n[info] Loading global plugins from /.sbt/0.13/plugins\n[info] Set current project to cuentaAB (in build file:tmp/cuentaAB/)\n[info] Updating {file:tmp/cuentaAB/}cuentaab...\n[info] Resolving org.scala-lang#scalap;2.10.5 ...\n[info] Done updating.\n[info] Including from cache: scala-library-2.10.5.jar\n[info] Checking every *.class/*.jar file's SHA-1.\n[info] Merging files...\n[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'\n[warn] Strategy 'discard' was applied to a file\n[info] SHA-1: fccf7195b12e31b9c7f956504a8341b65057e2e2\n[info] Packaging tmp/cuentaAB/target/scala-2.10/cuentaAB-assembly-1.0.jar ...\n[info] Done packaging.\n[success] Total time: 7 s, completed Sep 21, 2016 8:47:39 AM\n</code></pre>\n<ul>\n<li>Si se esta probando la aplicación en un entorno local, se puede evitar la creación de un fat JAR.</li>\n</ul>\n<pre><code class=\"console\">$ sbt package\n[info] Loading global plugins from .sbt/0.13/plugins\n[info] Set current project to cuentaAB (in build file:/tmp/cuentaAB/)\n[info] Packaging tmp/cuentaAB/target/scala-2.10/cuentaab_2.10-1.0.jar ...\n[info] Done packaging.\n[success] Total time: 0 s, completed Sep 21, 2016 8:50:35 AM\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1506720531915_-1262944884","id":"20160901-183722_1130198137","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:37"},{"text":"%md \n\n### Ejecución\n\n* Para enviar una aplicación a Spark usamos un script llamado `spark-submit`.\n\n* Formato general:\n\n    ```\n    bin/spark-submit [options] <app jar | python file> [app options]\n    ```\n* `[options]` es una lista de flags para `spark-submit`. Ver `spark-submit --help`.\n* `<app jar | python file>` es el JAR o el script de python a ejecutar.\n* `[app options]` son los argumentos que se pasan a la aplicación.\n* También se pueden setear configuraciones del SparkConf usando `--conf prop=value` o especificando un archivo de propiedades mediante `--properties-file`.\n* Cuando se ejecuta `spark-submit` sin nada mas que el nombre del JAR, se ejecuta la aplicación localmente.\n\n**Ejemplo:**\n<br>\n```console\n$ ./bin/spark-submit \\\n--master spark://hostname:7077 \\\n--deploy-mode cluster \\\n--class com.databricks.examples.SparkExample \\ --name \"Example Program\" \\\n--jars dep1.jar,dep2.jar,dep3.jar \\ --total-executor-cores 300 \\\n--executor-memory 10g \\\nmyApp.jar \"options\" \"to your application\" \"go here\"\n```","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Ejecución</h3>\n<ul>\n<li><p>Para enviar una aplicación a Spark usamos un script llamado <code>spark-submit</code>.</p>\n</li>\n<li><p>Formato general:</p>\n<pre><code>bin/spark-submit [options] &lt;app jar | python file&gt; [app options]\n</code></pre>\n</li>\n<li><p><code>[options]</code> es una lista de flags para <code>spark-submit</code>. Ver <code>spark-submit --help</code>.</p>\n</li>\n<li><p><code>&lt;app jar | python file&gt;</code> es el JAR o el script de python a ejecutar.</p>\n</li>\n<li><p><code>[app options]</code> son los argumentos que se pasan a la aplicación.</p>\n</li>\n<li><p>También se pueden setear configuraciones del SparkConf usando <code>--conf prop=value</code> o especificando un archivo de propiedades mediante <code>--properties-file</code>.</p>\n</li>\n<li><p>Cuando se ejecuta <code>spark-submit</code> sin nada mas que el nombre del JAR, se ejecuta la aplicación localmente.</p>\n</li>\n</ul>\n<p><strong>Ejemplo:</strong>\n<br  /><br></p>\n<pre><code class=\"console\">$ ./bin/spark-submit \\\n--master spark://hostname:7077 \\\n--deploy-mode cluster \\\n--class com.databricks.examples.SparkExample \\ --name \"Example Program\" \\\n--jars dep1.jar,dep2.jar,dep3.jar \\ --total-executor-cores 300 \\\n--executor-memory 10g \\\nmyApp.jar \"options\" \"to your application\" \"go here\"\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1506720531917_-1265253378","id":"20160921-002959_1896352202","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:38"},{"title":"FIN","text":"print(\"\"\"%html\n<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\n\"\"\")","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\n"}]},"apps":[],"jobName":"paragraph_1506720531918_-1264099131","id":"20160830-165045_1663894118","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:39"},{"text":"","dateUpdated":"2017-09-29T18:28:51-0300","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1506720531920_-1254095659","id":"20160830-165158_957810942","dateCreated":"2017-09-29T18:28:51-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:40"}],"name":"Presentación 05 - Aplicaciones Autónomas","id":"2CUXQ364N","angularObjects":{"2CQY683MD:shared_process":[],"2CRGPDJTN:shared_process":[],"2CTZD3XCX:shared_process":[],"2CRMCDEQM:shared_process":[],"2CTAV9E28:shared_process":[],"2CRA7KCDA:shared_process":[],"2CQ81X7G5:shared_process":[],"2CRUQ6EVN:shared_process":[],"2CTHYC7X9:shared_process":[],"2CRHWDFYM:shared_process":[],"2CQJ7MXW5:shared_process":[],"2CQAR5VSM:shared_process":[],"2CRCFF6HB:shared_process":[],"2CTK8TR8Q:shared_process":[],"2CQN5Q87B:shared_process":[],"2CTW5YZGW:shared_process":[],"2CSR7EJVT:shared_process":[],"2CRUQ7EXT:shared_process":[],"2CRFFBRXJ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}